{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs Sampling: Theory and Implementation from Scratch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Gibbs sampling is a powerful Markov Chain Monte Carlo (MCMC) method particularly well-suited for sampling from multivariate distributions when the conditional distributions of each variable (given all others) are known and easy to sample from. Named after physicist Josiah Willard Gibbs, this algorithm is widely used in Bayesian statistics, machine learning, and statistical physics.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Explore the theoretical foundations of the Gibbs sampling algorithm\n",
    "2. Implement the algorithm from scratch in Python\n",
    "3. Apply it to sample from a bivariate normal distribution\n",
    "4. Extend it to a more complex problem: Bayesian mixture model\n",
    "5. Analyze the algorithm's performance and limitations\n",
    "\n",
    "Let's begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical Background\n",
    "\n",
    "### 1.1 The Gibbs Sampling Algorithm\n",
    "\n",
    "Gibbs sampling is a special case of the Metropolis-Hastings algorithm where proposals are always accepted. It works by sampling each variable conditionally on all others. For a target joint distribution $p(x_1, x_2, \\ldots, x_n)$, the algorithm works as follows:\n",
    "\n",
    "1. Start with initial values for all variables $(x_1^{(0)}, x_2^{(0)}, \\ldots, x_n^{(0)})$\n",
    "2. For each iteration $t = 0, 1, 2, \\ldots$:\n",
    "   a. Sample $x_1^{(t+1)} \\sim p(x_1 | x_2^{(t)}, x_3^{(t)}, \\ldots, x_n^{(t)})$\n",
    "   b. Sample $x_2^{(t+1)} \\sim p(x_2 | x_1^{(t+1)}, x_3^{(t)}, \\ldots, x_n^{(t)})$\n",
    "   c. $\\ldots$\n",
    "   d. Sample $x_n^{(t+1)} \\sim p(x_n | x_1^{(t+1)}, x_2^{(t+1)}, \\ldots, x_{n-1}^{(t+1)})$\n",
    "\n",
    "The key insight is that we only need to know the conditional distributions $p(x_i | x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n)$ for each variable $x_i$, not the full joint distribution.\n",
    "\n",
    "### 1.2 Key Properties\n",
    "\n",
    "- **Always Accepts Proposals**: Unlike the Metropolis algorithm, Gibbs sampling always accepts the proposed moves, resulting in an acceptance rate of 100%.\n",
    "\n",
    "- **Requires Conditional Distributions**: The algorithm requires that we can sample from the conditional distribution of each variable given all others.\n",
    "\n",
    "- **Detailed Balance**: Gibbs sampling satisfies detailed balance, which guarantees that the stationary distribution of the Markov chain is the target distribution.\n",
    "\n",
    "- **Coordinate-wise Updates**: The algorithm updates one variable at a time, which can be inefficient when variables are highly correlated.\n",
    "\n",
    "### 1.3 Practical Considerations\n",
    "\n",
    "- **Burn-in Period**: Initial samples are discarded to allow the chain to reach its stationary distribution.\n",
    "\n",
    "- **Thinning**: To reduce autocorrelation, we might keep only every $k$-th sample.\n",
    "\n",
    "- **Multiple Chains**: Running multiple chains from different starting points helps assess convergence.\n",
    "\n",
    "- **Sampling Order**: The order in which variables are updated can affect efficiency. Options include:\n",
    "  - Systematic scan: Update variables in a fixed order\n",
    "  - Random scan: Randomly select variables to update\n",
    "  - Blocked Gibbs: Update groups of variables together\n",
    "\n",
    "Now, let's implement the Gibbs sampling algorithm from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation from Scratch\n",
    "\n",
    "We'll start by implementing a general Gibbs sampler that can work with any set of conditional sampling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def gibbs_sampler(conditional_samplers, initial_state, n_samples, burn_in=0, thin=1):\n",
    "    \"\"\"\n",
    "    Gibbs sampling algorithm for sampling from a multivariate distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    conditional_samplers : list of functions\n",
    "        List of functions that sample from conditional distributions.\n",
    "        Each function takes the current state and returns a new value for its variable.\n",
    "    initial_state : array-like\n",
    "        Initial state of the Markov chain\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    burn_in : int, optional\n",
    "        Number of initial samples to discard\n",
    "    thin : int, optional\n",
    "        Thinning factor (keep every thin-th sample)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : array\n",
    "        Generated samples\n",
    "    \"\"\"\n",
    "    # Convert initial state to numpy array if it isn't already\n",
    "    current_state = np.array(initial_state, dtype=float)\n",
    "    dim = current_state.shape[0]  # Dimensionality of the state\n",
    "    \n",
    "    # Total number of iterations needed\n",
    "    n_iterations = burn_in + thin * n_samples\n",
    "    \n",
    "    # Initialize storage for samples\n",
    "    samples = np.zeros((n_samples, dim))\n",
    "    \n",
    "    # Main sampling loop\n",
    "    sample_idx = 0\n",
    "    for i in tqdm(range(n_iterations), desc=\"Sampling\"):\n",
    "        # Update each variable using its conditional sampler\n",
    "        for j, sampler in enumerate(conditional_samplers):\n",
    "            current_state[j] = sampler(current_state)\n",
    "        \n",
    "        # Store the sample if past burn-in and due for storage based on thinning\n",
    "        if i >= burn_in and (i - burn_in) % thin == 0:\n",
    "            samples[sample_idx] = current_state\n",
    "            sample_idx += 1\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example 1: Sampling from a Bivariate Normal Distribution\n",
    "\n",
    "Let's apply our Gibbs sampler to a bivariate normal distribution. This is a good test case because:\n",
    "1. The conditional distributions are known analytically (they are univariate normals)\n",
    "2. We can visualize the results in 2D\n",
    "3. It demonstrates how Gibbs sampling moves parallel to the axes\n",
    "4. It shows how correlation affects mixing and efficiency\n",
    "\n",
    "For a bivariate normal distribution with mean $\\mu = [\\mu_1, \\mu_2]$ and covariance matrix $\\Sigma = \\begin{bmatrix} \\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2 \\end{bmatrix}$, the conditional distributions are:\n",
    "\n",
    "$$p(x_1 | x_2) = \\mathcal{N}\\left(\\mu_1 + \\rho\\frac{\\sigma_1}{\\sigma_2}(x_2 - \\mu_2), \\sigma_1^2(1-\\rho^2)\\right)$$\n",
    "\n",
    "$$p(x_2 | x_1) = \\mathcal{N}\\left(\\mu_2 + \\rho\\frac{\\sigma_2}{\\sigma_1}(x_1 - \\mu_1), \\sigma_2^2(1-\\rho^2)\\right)$$\n",
    "\n",
    "Let's implement these conditional samplers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_bivariate_normal_conditional_samplers(mean, cov):\n",
    "    \"\"\"\n",
    "    Create conditional samplers for a bivariate normal distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mean : array-like\n",
    "        Mean vector [mu_1, mu_2]\n",
    "    cov : array-like\n",
    "        Covariance matrix [[sigma_1^2, rho*sigma_1*sigma_2], [rho*sigma_1*sigma_2, sigma_2^2]]\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    samplers : list of functions\n",
    "        Conditional samplers for x_1|x_2 and x_2|x_1\n",
    "    \"\"\"\n",
    "    mu_1, mu_2 = mean\n",
    "    sigma_1_sq, rho_sigma_1_sigma_2, _, sigma_2_sq = cov.flatten()\n",
    "    \n",
    "    sigma_1 = np.sqrt(sigma_1_sq)\n",
    "    sigma_2 = np.sqrt(sigma_2_sq)\n",
    "    rho = rho_sigma_1_sigma_2 / (sigma_1 * sigma_2)\n",
    "    \n",
    "    # Conditional standard deviations\n",
    "    cond_std_1 = sigma_1 * np.sqrt(1 - rho**2)\n",
    "    cond_std_2 = sigma_2 * np.sqrt(1 - rho**2)\n",
    "    \n",
    "    # Conditional sampler for x_1|x_2\n",
    "    def sample_x1_given_x2(state):\n",
    "        x2 = state[1]\n",
    "        cond_mean_1 = mu_1 + rho * (sigma_1 / sigma_2) * (x2 - mu_2)\n",
    "        return np.random.normal(cond_mean_1, cond_std_1)\n",
    "    \n",
    "    # Conditional sampler for x_2|x_1\n",
    "    def sample_x2_given_x1(state):\n",
    "        x1 = state[0]\n",
    "        cond_mean_2 = mu_2 + rho * (sigma_2 / sigma_1) * (x1 - mu_1)\n",
    "        return np.random.normal(cond_mean_2, cond_std_2)\n",
    "    \n",
    "    return [sample_x1_given_x2, sample_x2_given_x1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the Gibbs sampler for a bivariate normal distribution with correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define parameters for the target distribution\n",
    "mean = np.array([0, 0])\n",
    "rho = 0.8  # Correlation coefficient\n",
    "cov = np.array([[1, rho], [rho, 1]])\n",
    "\n",
    "# Create conditional samplers\n",
    "conditional_samplers = create_bivariate_normal_conditional_samplers(mean, cov)\n",
    "\n",
    "# Set sampling parameters\n",
    "initial_state = np.array([2, 2])  # Start away from the mean\n",
    "n_samples = 5000\n",
    "burn_in = 1000\n",
    "thin = 1\n",
    "\n",
    "# Run the Gibbs sampler\n",
    "samples = gibbs_sampler(conditional_samplers, initial_state, n_samples, burn_in, thin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the results to see how well our sampler approximates the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_2d_samples(samples, true_mean=None, true_cov=None, title=\"Samples\", show_trajectory=False, n_trajectory=100):\n",
    "    \"\"\"\n",
    "    Plot 2D samples with marginal distributions and optionally compare to true distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    samples : array-like\n",
    "        2D samples to plot\n",
    "    true_mean : array-like, optional\n",
    "        True mean vector for comparison\n",
    "    true_cov : array-like, optional\n",
    "        True covariance matrix for comparison\n",
    "    title : str, optional\n",
    "        Plot title\n",
    "    show_trajectory : bool, optional\n",
    "        Whether to show the trajectory of the first n_trajectory samples\n",
    "    n_trajectory : int, optional\n",
    "        Number of samples to include in trajectory plot\n",
    "    \"\"\"\n",
    "    # Create a figure with a grid for the joint and marginal plots\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    gs = fig.add_gridspec(2, 2, width_ratios=[3, 1], height_ratios=[1, 3],\n",
    "                         wspace=0.05, hspace=0.05)\n",
    "    \n",
    "    # Joint distribution plot\n",
    "    ax_joint = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    # Plot the samples\n",
    "    ax_joint.scatter(samples[:, 0], samples[:, 1], alpha=0.5, s=5)\n",
    "    \n",
    "    # If requested, show the trajectory of the first n_trajectory samples\n",
    "    if show_trajectory and n_trajectory > 0:\n",
    "        n = min(n_trajectory, len(samples))\n",
    "        ax_joint.plot(samples[:n, 0], samples[:n, 1], 'r-', alpha=0.5, linewidth=0.5)\n",
    "        ax_joint.scatter(samples[0, 0], samples[0, 1], color='red', s=30, label='Start')\n",
    "    \n",
    "    # If true distribution is provided, plot contours\n",
    "    if true_mean is not None and true_cov is not None:\n",
    "        # Create a grid of points\n",
    "        x = np.linspace(-3, 3, 100)\n",
    "        y = np.linspace(-3, 3, 100)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        pos = np.dstack((X, Y))\n",
    "        \n",
    "        # Compute PDF values on the grid\n",
    "        rv = stats.multivariate_normal(true_mean, true_cov)\n",
    "        Z = rv.pdf(pos)\n",
    "        \n",
    "        # Plot contours\n",
    "        levels = np.linspace(0, Z.max(), 10)[1:]\n",
    "        ax_joint.contour(X, Y, Z, levels=levels, colors='r', alpha=0.7)\n",
    "    \n",
    "    # Set labels and limits\n",
    "    ax_joint.set_xlabel('$x_1$')\n",
    "    ax_joint.set_ylabel('$x_2$')\n",
    "    ax_joint.set_xlim(-3, 3)\n",
    "    ax_joint.set_ylim(-3, 3)\n",
    "    \n",
    "    # Marginal distribution for x1\n",
    "    ax_marg_x = fig.add_subplot(gs[0, 0], sharex=ax_joint)\n",
    "    sns.kdeplot(samples[:, 0], ax=ax_marg_x, fill=True)\n",
    "    if true_mean is not None and true_cov is not None:\n",
    "        x = np.linspace(-3, 3, 1000)\n",
    "        ax_marg_x.plot(x, stats.norm.pdf(x, true_mean[0], np.sqrt(true_cov[0, 0])), 'r')\n",
    "    ax_marg_x.set_yticks([])\n",
    "    ax_marg_x.set_title(title)\n",
    "    \n",
    "    # Marginal distribution for x2\n",
    "    ax_marg_y = fig.add_subplot(gs[1, 1], sharey=ax_joint)\n",
    "    sns.kdeplot(y=samples[:, 1], ax=ax_marg_y, fill=True)\n",
    "    if true_mean is not None and true_cov is not None:\n",
    "        y = np.linspace(-3, 3, 1000)\n",
    "        ax_marg_y.plot(stats.norm.pdf(y, true_mean[1], np.sqrt(true_cov[1, 1])), y, 'r')\n",
    "    ax_marg_y.set_xticks([])\n",
    "    \n",
    "    # Turn off tick labels on the marginal plots\n",
    "    plt.setp(ax_marg_x.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax_marg_y.get_yticklabels(), visible=False)\n",
    "    \n",
    "    if show_trajectory:\n",
    "        ax_joint.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot the samples\n",
    "plot_2d_samples(samples, mean, cov, title=f\"Gibbs Samples (ρ={rho})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot with trajectory to show the characteristic \"right-angle\" moves of Gibbs sampling\n",
    "plot_2d_samples(samples, mean, cov, title=f\"Gibbs Sampling Trajectory (ρ={rho})\", show_trajectory=True, n_trajectory=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the trace plots and autocorrelation to assess mixing and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_diagnostics(samples, parameter_names=None):\n",
    "    \"\"\"\n",
    "    Plot trace plots and autocorrelation for MCMC samples.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    samples : array-like\n",
    "        MCMC samples\n",
    "    parameter_names : list, optional\n",
    "        Names of the parameters\n",
    "    \"\"\"\n",
    "    n_samples, dim = samples.shape\n",
    "    \n",
    "    if parameter_names is None:\n",
    "        parameter_names = [f\"$x_{i+1}$\" for i in range(dim)]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(dim, 2, figsize=(12, 3*dim))\n",
    "    \n",
    "    # Plot trace and autocorrelation for each parameter\n",
    "    for i in range(dim):\n",
    "        # Trace plot\n",
    "        axes[i, 0].plot(samples[:, i])\n",
    "        axes[i, 0].set_title(f\"Trace plot for {parameter_names[i]}\")\n",
    "        axes[i, 0].set_xlabel(\"Iteration\")\n",
    "        axes[i, 0].set_ylabel(parameter_names[i])\n",
    "        \n",
    "        # Autocorrelation plot\n",
    "        lags = min(50, n_samples // 5)\n",
    "        acf = np.zeros(lags)\n",
    "        for lag in range(lags):\n",
    "            acf[lag] = np.corrcoef(samples[lag:, i], samples[:-lag if lag > 0 else None, i])[0, 1]\n",
    "        \n",
    "        axes[i, 1].bar(range(lags), acf)\n",
    "        axes[i, 1].set_title(f\"Autocorrelation for {parameter_names[i]}\")\n",
    "        axes[i, 1].set_xlabel(\"Lag\")\n",
    "        axes[i, 1].set_ylabel(\"Autocorrelation\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot diagnostics\n",
    "plot_diagnostics(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Effect of Correlation\n",
    "\n",
    "The correlation in the target distribution significantly affects the efficiency of Gibbs sampling. Let's experiment with different correlation values to see how they impact the mixing of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to calculate effective sample size (ESS)\n",
    "def calculate_ess(chain):\n",
    "    \"\"\"\n",
    "    Calculate effective sample size using autocorrelation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    chain : array-like\n",
    "        MCMC chain\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ess : float\n",
    "        Effective sample size\n",
    "    \"\"\"\n",
    "    n = len(chain)\n",
    "    lags = min(50, n // 5)\n",
    "    acf = np.zeros(lags)\n",
    "    for lag in range(lags):\n",
    "        acf[lag] = np.corrcoef(chain[lag:], chain[:-lag if lag > 0 else None])[0, 1]\n",
    "    # Truncate at first negative autocorrelation\n",
    "    neg_idx = np.where(acf < 0)[0]\n",
    "    if len(neg_idx) > 0:\n",
    "        acf = acf[:neg_idx[0]]\n",
    "    # Calculate ESS\n",
    "    ess = n / (1 + 2 * np.sum(acf[1:]))\n",
    "    return ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Try different correlation values\n",
    "rho_values = [0.0, 0.5, 0.9, 0.99]\n",
    "corr_results = []\n",
    "\n",
    "for rho in rho_values:\n",
    "    # Update covariance matrix\n",
    "    cov = np.array([[1, rho], [rho, 1]])\n",
    "    \n",
    "    # Create conditional samplers\n",
    "    conditional_samplers = create_bivariate_normal_conditional_samplers(mean, cov)\n",
    "    \n",
    "    # Run sampler\n",
    "    samples = gibbs_sampler(conditional_samplers, initial_state, n_samples=2000, burn_in=500, thin=1)\n",
    "    \n",
    "    # Calculate effective sample size\n",
    "    ess_x1 = calculate_ess(samples[:, 0])\n",
    "    ess_x2 = calculate_ess(samples[:, 1])\n",
    "    ess_avg = (ess_x1 + ess_x2) / 2\n",
    "    \n",
    "    corr_results.append({\n",
    "        'rho': rho,\n",
    "        'ess_x1': ess_x1,\n",
    "        'ess_x2': ess_x2,\n",
    "        'ess_avg': ess_avg,\n",
    "        'efficiency': ess_avg / 2000  # ESS per iteration\n",
    "    })\n",
    "    \n",
    "    # Plot samples\n",
    "    plot_2d_samples(samples, mean, cov, \n",
    "                   title=f\"Correlation ρ={rho}, ESS={ess_avg:.1f}\", \n",
    "                   show_trajectory=True, n_trajectory=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Summarize correlation results\n",
    "corr_results_df = pd.DataFrame(corr_results)\n",
    "corr_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot efficiency vs correlation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(corr_results_df['rho'], corr_results_df['efficiency'], 'o-')\n",
    "plt.xlabel('Correlation (ρ)')\n",
    "plt.ylabel('Sampling Efficiency (ESS/N)')\n",
    "plt.title('Effect of Target Distribution Correlation on Gibbs Sampling')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Comparison with Metropolis Algorithm\n",
    "\n",
    "Let's compare the performance of Gibbs sampling with the Metropolis algorithm for the same bivariate normal distribution. We'll implement a simple Metropolis sampler for this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def metropolis_sampler(log_prob_func, initial_state, n_samples, proposal_std, burn_in=0, thin=1):\n",
    "    \"\"\"\n",
    "    Metropolis algorithm for sampling from a probability distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    log_prob_func : function\n",
    "        Function that computes the log probability of a state\n",
    "    initial_state : array-like\n",
    "        Initial state of the Markov chain\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    proposal_std : float or array-like\n",
    "        Standard deviation of the Gaussian proposal distribution\n",
    "    burn_in : int, optional\n",
    "        Number of initial samples to discard\n",
    "    thin : int, optional\n",
    "        Thinning factor (keep every thin-th sample)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : array\n",
    "        Generated samples\n",
    "    acceptance_rate : float\n",
    "        Fraction of proposals that were accepted\n",
    "    \"\"\"\n",
    "    # Convert initial state to numpy array if it isn't already\n",
    "    current_state = np.array(initial_state, dtype=float)\n",
    "    dim = current_state.shape[0]  # Dimensionality of the state\n",
    "    \n",
    "    # Total number of iterations needed\n",
    "    n_iterations = burn_in + thin * n_samples\n",
    "    \n",
    "    # Initialize storage for samples and acceptance tracking\n",
    "    samples = np.zeros((n_samples, dim))\n",
    "    current_log_prob = log_prob_func(current_state)\n",
    "    n_accepted = 0\n",
    "    \n",
    "    # Main sampling loop\n",
    "    sample_idx = 0\n",
    "    for i in tqdm(range(n_iterations), desc=\"Sampling\"):\n",
    "        # Propose a new state\n",
    "        proposal = current_state + np.random.normal(0, proposal_std, size=dim)\n",
    "        \n",
    "        # Compute log probability of the proposed state\n",
    "        proposal_log_prob = log_prob_func(proposal)\n",
    "        \n",
    "        # Compute log acceptance ratio\n",
    "        log_acceptance_ratio = proposal_log_prob - current_log_prob\n",
    "        \n",
    "        # Accept or reject the proposal\n",
    "        if np.log(np.random.random()) < log_acceptance_ratio:\n",
    "            current_state = proposal\n",
    "            current_log_prob = proposal_log_prob\n",
    "            n_accepted += 1\n",
    "        \n",
    "        # Store the sample if past burn-in and due for storage based on thinning\n",
    "        if i >= burn_in and (i - burn_in) % thin == 0:\n",
    "            samples[sample_idx] = current_state\n",
    "            sample_idx += 1\n",
    "    \n",
    "    # Compute acceptance rate\n",
    "    acceptance_rate = n_accepted / n_iterations\n",
    "    \n",
    "    return samples, acceptance_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def multivariate_normal_log_pdf(x, mean, cov):\n",
    "    \"\"\"\n",
    "    Compute the log PDF of a multivariate normal distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        Point at which to evaluate the PDF\n",
    "    mean : array-like\n",
    "        Mean vector of the distribution\n",
    "    cov : array-like\n",
    "        Covariance matrix of the distribution\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    log_pdf : float\n",
    "        Log probability density at x\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    mean = np.array(mean)\n",
    "    cov = np.array(cov)\n",
    "    dim = len(x)\n",
    "    \n",
    "    # Compute log determinant of covariance matrix\n",
    "    sign, logdet = np.linalg.slogdet(cov)\n",
    "    \n",
    "    # Compute Mahalanobis distance\n",
    "    diff = x - mean\n",
    "    mahalanobis = diff.T @ np.linalg.inv(cov) @ diff\n",
    "    \n",
    "    # Compute log PDF\n",
    "    log_pdf = -0.5 * (dim * np.log(2 * np.pi) + logdet + mahalanobis)\n",
    "    \n",
    "    return log_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare Gibbs and Metropolis for different correlation values\n",
    "rho_values = [0.0, 0.5, 0.9, 0.99]\n",
    "comparison_results = []\n",
    "\n",
    "for rho in rho_values:\n",
    "    # Update covariance matrix\n",
    "    cov = np.array([[1, rho], [rho, 1]])\n",
    "    \n",
    "    # Create target log probability function for Metropolis\n",
    "    def target_log_prob(x):\n",
    "        return multivariate_normal_log_pdf(x, mean, cov)\n",
    "    \n",
    "    # Create conditional samplers for Gibbs\n",
    "    conditional_samplers = create_bivariate_normal_conditional_samplers(mean, cov)\n",
    "    \n",
    "    # Run Gibbs sampler\n",
    "    gibbs_samples = gibbs_sampler(conditional_samplers, initial_state, n_samples=2000, burn_in=500, thin=1)\n",
    "    \n",
    "    # Run Metropolis sampler\n",
    "    metropolis_samples, acceptance_rate = metropolis_sampler(\n",
    "        target_log_prob, initial_state, n_samples=2000, proposal_std=0.5, burn_in=500, thin=1\n",
    "    )\n",
    "    \n",
    "    # Calculate effective sample size for Gibbs\n",
    "    gibbs_ess_x1 = calculate_ess(gibbs_samples[:, 0])\n",
    "    gibbs_ess_x2 = calculate_ess(gibbs_samples[:, 1])\n",
    "    gibbs_ess_avg = (gibbs_ess_x1 + gibbs_ess_x2) / 2\n",
    "    \n",
    "    # Calculate effective sample size for Metropolis\n",
    "    metropolis_ess_x1 = calculate_ess(metropolis_samples[:, 0])\n",
    "    metropolis_ess_x2 = calculate_ess(metropolis_samples[:, 1])\n",
    "    metropolis_ess_avg = (metropolis_ess_x1 + metropolis_ess_x2) / 2\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'rho': rho,\n",
    "        'gibbs_ess_avg': gibbs_ess_avg,\n",
    "        'gibbs_efficiency': gibbs_ess_avg / 2000,\n",
    "        'metropolis_ess_avg': metropolis_ess_avg,\n",
    "        'metropolis_efficiency': metropolis_ess_avg / 2000,\n",
    "        'metropolis_acceptance_rate': acceptance_rate\n",
    "    })\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot Gibbs samples\n",
    "    axes[0].scatter(gibbs_samples[:, 0], gibbs_samples[:, 1], alpha=0.5, s=5)\n",
    "    axes[0].set_title(f\"Gibbs Sampling (ρ={rho}, ESS={gibbs_ess_avg:.1f})\")\n",
    "    axes[0].set_xlabel('$x_1$')\n",
    "    axes[0].set_ylabel('$x_2$')\n",
    "    axes[0].set_xlim(-3, 3)\n",
    "    axes[0].set_ylim(-3, 3)\n",
    "    \n",
    "    # Plot Metropolis samples\n",
    "    axes[1].scatter(metropolis_samples[:, 0], metropolis_samples[:, 1], alpha=0.5, s=5)\n",
    "    axes[1].set_title(f\"Metropolis (ρ={rho}, ESS={metropolis_ess_avg:.1f}, Accept={acceptance_rate:.2f})\")\n",
    "    axes[1].set_xlabel('$x_1$')\n",
    "    axes[1].set_ylabel('$x_2$')\n",
    "    axes[1].set_xlim(-3, 3)\n",
    "    axes[1].set_ylim(-3, 3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Summarize comparison results\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot efficiency comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(comparison_df['rho'], comparison_df['gibbs_efficiency'], 'o-', label='Gibbs')\n",
    "plt.plot(comparison_df['rho'], comparison_df['metropolis_efficiency'], 'o-', label='Metropolis')\n",
    "plt.xlabel('Correlation (ρ)')\n",
    "plt.ylabel('Sampling Efficiency (ESS/N)')\n",
    "plt.title('Sampling Efficiency: Gibbs vs Metropolis')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example 2: Bayesian Mixture Model\n",
    "\n",
    "Now, let's apply Gibbs sampling to a more complex problem: a Bayesian mixture model. This is a classic application where Gibbs sampling is particularly effective due to the natural conditional structure of the model.\n",
    "\n",
    "We'll implement a Gaussian mixture model with $K$ components. The model is:\n",
    "\n",
    "$$p(x_i | z_i, \\mu, \\sigma^2) = \\mathcal{N}(x_i | \\mu_{z_i}, \\sigma^2_{z_i})$$\n",
    "$$p(z_i | \\pi) = \\text{Categorical}(z_i | \\pi)$$\n",
    "$$p(\\mu_k) = \\mathcal{N}(\\mu_k | \\mu_0, \\sigma^2_0)$$\n",
    "$$p(\\sigma^2_k) = \\text{InverseGamma}(\\sigma^2_k | \\alpha, \\beta)$$\n",
    "$$p(\\pi) = \\text{Dirichlet}(\\pi | \\alpha_1, \\ldots, \\alpha_K)$$\n",
    "\n",
    "where:\n",
    "- $x_i$ are the observed data points\n",
    "- $z_i$ are the latent component assignments\n",
    "- $\\mu_k$ are the component means\n",
    "- $\\sigma^2_k$ are the component variances\n",
    "- $\\pi$ are the component weights\n",
    "\n",
    "### 4.1 Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set true parameter values\n",
    "true_means = np.array([-2.0, 2.0, 6.0])  # Component means\n",
    "true_stds = np.array([0.5, 0.8, 1.2])    # Component standard deviations\n",
    "true_weights = np.array([0.3, 0.5, 0.2])  # Component weights\n",
    "n_components = len(true_means)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_data = 500\n",
    "true_components = np.random.choice(n_components, size=n_data, p=true_weights)\n",
    "data = np.zeros(n_data)\n",
    "\n",
    "for i in range(n_data):\n",
    "    component = true_components[i]\n",
    "    data[i] = np.random.normal(true_means[component], true_stds[component])\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(data, bins=30, density=True, alpha=0.6)\n",
    "\n",
    "# Plot the true mixture components\n",
    "x = np.linspace(-6, 10, 1000)\n",
    "mixture_pdf = np.zeros_like(x)\n",
    "\n",
    "for i in range(n_components):\n",
    "    component_pdf = true_weights[i] * stats.norm.pdf(x, true_means[i], true_stds[i])\n",
    "    mixture_pdf += component_pdf\n",
    "    plt.plot(x, component_pdf, '--', label=f'Component {i+1}')\n",
    "\n",
    "plt.plot(x, mixture_pdf, 'k-', linewidth=2, label='Mixture PDF')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Synthetic Data from Gaussian Mixture Model')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Implement Gibbs Sampler for Mixture Model\n",
    "\n",
    "Now, let's implement the Gibbs sampler for the mixture model. We need to derive the conditional distributions for each parameter:\n",
    "\n",
    "1. Component assignments $z_i | x_i, \\mu, \\sigma^2, \\pi$\n",
    "2. Component means $\\mu_k | x, z, \\sigma^2, \\pi$\n",
    "3. Component variances $\\sigma^2_k | x, z, \\mu, \\pi$\n",
    "4. Component weights $\\pi | x, z, \\mu, \\sigma^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def gibbs_sampler_mixture_model(data, n_components, n_samples, burn_in=0, thin=1):\n",
    "    \"\"\"\n",
    "    Gibbs sampler for a Gaussian mixture model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Observed data\n",
    "    n_components : int\n",
    "        Number of mixture components\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    burn_in : int, optional\n",
    "        Number of initial samples to discard\n",
    "    thin : int, optional\n",
    "        Thinning factor (keep every thin-th sample)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : dict\n",
    "        Dictionary containing samples for each parameter\n",
    "    \"\"\"\n",
    "    n_data = len(data)\n",
    "    \n",
    "    # Total number of iterations needed\n",
    "    n_iterations = burn_in + thin * n_samples\n",
    "    \n",
    "    # Initialize storage for samples\n",
    "    samples = {\n",
    "        'means': np.zeros((n_samples, n_components)),\n",
    "        'stds': np.zeros((n_samples, n_components)),\n",
    "        'weights': np.zeros((n_samples, n_components)),\n",
    "        'components': np.zeros((n_samples, n_data), dtype=int)\n",
    "    }\n",
    "    \n",
    "    # Initialize parameters\n",
    "    # Use K-means to get initial values\n",
    "    from sklearn.cluster import KMeans\n",
    "    kmeans = KMeans(n_clusters=n_components, random_state=42).fit(data.reshape(-1, 1))\n",
    "    \n",
    "    current_means = kmeans.cluster_centers_.flatten()\n",
    "    current_stds = np.ones(n_components)\n",
    "    current_weights = np.ones(n_components) / n_components\n",
    "    current_components = kmeans.labels_\n",
    "    \n",
    "    # Set hyperparameters for priors\n",
    "    # Mean prior: N(mu_0, sigma_0^2)\n",
    "    mu_0 = np.mean(data)\n",
    "    sigma_0 = np.std(data)\n",
    "    \n",
    "    # Variance prior: InverseGamma(alpha, beta)\n",
    "    alpha = 2.0\n",
    "    beta = 1.0\n",
    "    \n",
    "    # Weight prior: Dirichlet(alpha_1, ..., alpha_K)\n",
    "    alpha_weights = np.ones(n_components)\n",
    "    \n",
    "    # Main sampling loop\n",
    "    sample_idx = 0\n",
    "    for i in tqdm(range(n_iterations), desc=\"Sampling\"):\n",
    "        # 1. Sample component assignments\n",
    "        for j in range(n_data):\n",
    "            # Compute log probabilities for each component\n",
    "            log_probs = np.log(current_weights) + \\\n",
    "                        stats.norm.logpdf(data[j], current_means, current_stds)\n",
    "            \n",
    "            # Normalize to get probabilities\n",
    "            log_probs -= np.max(log_probs)  # For numerical stability\n",
    "            probs = np.exp(log_probs)\n",
    "            probs /= np.sum(probs)\n",
    "            \n",
    "            # Sample new component\n",
    "            current_components[j] = np.random.choice(n_components, p=probs)\n",
    "        \n",
    "        # 2. Sample component means\n",
    "        for k in range(n_components):\n",
    "            # Get data points assigned to this component\n",
    "            mask = (current_components == k)\n",
    "            n_k = np.sum(mask)\n",
    "            \n",
    "            if n_k > 0:\n",
    "                # Compute posterior parameters\n",
    "                x_bar = np.mean(data[mask]) if n_k > 0 else 0\n",
    "                sigma_k = current_stds[k]\n",
    "                \n",
    "                # Posterior precision (1/variance)\n",
    "                precision_0 = 1.0 / (sigma_0 ** 2)\n",
    "                precision_k = n_k / (sigma_k ** 2)\n",
    "                precision_posterior = precision_0 + precision_k\n",
    "                \n",
    "                # Posterior mean\n",
    "                mean_posterior = (precision_0 * mu_0 + precision_k * x_bar) / precision_posterior\n",
    "                \n",
    "                # Posterior standard deviation\n",
    "                std_posterior = np.sqrt(1.0 / precision_posterior)\n",
    "                \n",
    "                # Sample new mean\n",
    "                current_means[k] = np.random.normal(mean_posterior, std_posterior)\n",
    "        \n",
    "        # 3. Sample component variances\n",
    "        for k in range(n_components):\n",
    "            # Get data points assigned to this component\n",
    "            mask = (current_components == k)\n",
    "            n_k = np.sum(mask)\n",
    "            \n",
    "            if n_k > 0:\n",
    "                # Compute sum of squared deviations\n",
    "                sum_sq_dev = np.sum((data[mask] - current_means[k]) ** 2)\n",
    "                \n",
    "                # Posterior shape and scale parameters\n",
    "                alpha_posterior = alpha + n_k / 2.0\n",
    "                beta_posterior = beta + sum_sq_dev / 2.0\n",
    "                \n",
    "                # Sample new variance\n",
    "                current_stds[k] = np.sqrt(1.0 / np.random.gamma(alpha_posterior, 1.0 / beta_posterior))\n",
    "        \n",
    "        # 4. Sample component weights\n",
    "        # Count number of data points in each component\n",
    "        component_counts = np.zeros(n_components)\n",
    "        for k in range(n_components):\n",
    "            component_counts[k] = np.sum(current_components == k)\n",
    "        \n",
    "        # Sample from Dirichlet distribution\n",
    "        alpha_posterior = alpha_weights + component_counts\n",
    "        current_weights = np.random.dirichlet(alpha_posterior)\n",
    "        \n",
    "        # Store the sample if past burn-in and due for storage based on thinning\n",
    "        if i >= burn_in and (i - burn_in) % thin == 0:\n",
    "            samples['means'][sample_idx] = current_means\n",
    "            samples['stds'][sample_idx] = current_stds\n",
    "            samples['weights'][sample_idx] = current_weights\n",
    "            samples['components'][sample_idx] = current_components\n",
    "            sample_idx += 1\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run the Gibbs sampler for the mixture model\n",
    "n_samples = 2000\n",
    "burn_in = 500\n",
    "thin = 2\n",
    "\n",
    "mixture_samples = gibbs_sampler_mixture_model(data, n_components, n_samples, burn_in, thin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot trace plots for means, stds, and weights\n",
    "fig, axes = plt.subplots(3, n_components, figsize=(15, 10))\n",
    "\n",
    "for k in range(n_components):\n",
    "    # Means\n",
    "    axes[0, k].plot(mixture_samples['means'][:, k])\n",
    "    axes[0, k].axhline(true_means[k], color='r', linestyle='--')\n",
    "    axes[0, k].set_title(f'Mean {k+1}')\n",
    "    \n",
    "    # Standard deviations\n",
    "    axes[1, k].plot(mixture_samples['stds'][:, k])\n",
    "    axes[1, k].axhline(true_stds[k], color='r', linestyle='--')\n",
    "    axes[1, k].set_title(f'Std {k+1}')\n",
    "    \n",
    "    # Weights\n",
    "    axes[2, k].plot(mixture_samples['weights'][:, k])\n",
    "    axes[2, k].axhline(true_weights[k], color='r', linestyle='--')\n",
    "    axes[2, k].set_title(f'Weight {k+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot posterior distributions for means, stds, and weights\n",
    "fig, axes = plt.subplots(3, n_components, figsize=(15, 10))\n",
    "\n",
    "for k in range(n_components):\n",
    "    # Means\n",
    "    sns.histplot(mixture_samples['means'][:, k], kde=True, ax=axes[0, k])\n",
    "    axes[0, k].axvline(true_means[k], color='r', linestyle='--')\n",
    "    axes[0, k].set_title(f'Mean {k+1}')\n",
    "    \n",
    "    # Standard deviations\n",
    "    sns.histplot(mixture_samples['stds'][:, k], kde=True, ax=axes[1, k])\n",
    "    axes[1, k].axvline(true_stds[k], color='r', linestyle='--')\n",
    "    axes[1, k].set_title(f'Std {k+1}')\n",
    "    \n",
    "    # Weights\n",
    "    sns.histplot(mixture_samples['weights'][:, k], kde=True, ax=axes[2, k])\n",
    "    axes[2, k].axvline(true_weights[k], color='r', linestyle='--')\n",
    "    axes[2, k].set_title(f'Weight {k+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compute posterior means\n",
    "posterior_means = np.mean(mixture_samples['means'], axis=0)\n",
    "posterior_stds = np.mean(mixture_samples['stds'], axis=0)\n",
    "posterior_weights = np.mean(mixture_samples['weights'], axis=0)\n",
    "\n",
    "# Sort components by mean for easier comparison\n",
    "sort_idx = np.argsort(posterior_means)\n",
    "posterior_means = posterior_means[sort_idx]\n",
    "posterior_stds = posterior_stds[sort_idx]\n",
    "posterior_weights = posterior_weights[sort_idx]\n",
    "\n",
    "# Sort true parameters in the same way\n",
    "true_sort_idx = np.argsort(true_means)\n",
    "true_means_sorted = true_means[true_sort_idx]\n",
    "true_stds_sorted = true_stds[true_sort_idx]\n",
    "true_weights_sorted = true_weights[true_sort_idx]\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Parameter': ['Mean 1', 'Mean 2', 'Mean 3', 'Std 1', 'Std 2', 'Std 3', 'Weight 1', 'Weight 2', 'Weight 3'],\n",
    "    'True Value': np.concatenate([true_means_sorted, true_stds_sorted, true_weights_sorted]),\n",
    "    'Posterior Mean': np.concatenate([posterior_means, posterior_stds, posterior_weights])\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot the data with the fitted mixture model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(data, bins=30, density=True, alpha=0.6, label='Data')\n",
    "\n",
    "# Plot the true mixture components\n",
    "x = np.linspace(-6, 10, 1000)\n",
    "true_mixture_pdf = np.zeros_like(x)\n",
    "fitted_mixture_pdf = np.zeros_like(x)\n",
    "\n",
    "for i in range(n_components):\n",
    "    # True components\n",
    "    true_component_pdf = true_weights_sorted[i] * stats.norm.pdf(x, true_means_sorted[i], true_stds_sorted[i])\n",
    "    true_mixture_pdf += true_component_pdf\n",
    "    plt.plot(x, true_component_pdf, '--', alpha=0.5, label=f'True Component {i+1}')\n",
    "    \n",
    "    # Fitted components\n",
    "    fitted_component_pdf = posterior_weights[i] * stats.norm.pdf(x, posterior_means[i], posterior_stds[i])\n",
    "    fitted_mixture_pdf += fitted_component_pdf\n",
    "    plt.plot(x, fitted_component_pdf, '-', alpha=0.5, label=f'Fitted Component {i+1}')\n",
    "\n",
    "plt.plot(x, true_mixture_pdf, 'k--', linewidth=2, label='True Mixture PDF')\n",
    "plt.plot(x, fitted_mixture_pdf, 'r-', linewidth=2, label='Fitted Mixture PDF')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Gaussian Mixture Model: True vs Fitted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Label Switching\n",
    "\n",
    "One challenge in Bayesian mixture models is the \"label switching\" problem, where the component labels can switch during sampling due to the symmetry of the model. Let's examine this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot means over iterations to check for label switching\n",
    "plt.figure(figsize=(10, 6))\n",
    "for k in range(n_components):\n",
    "    plt.plot(mixture_samples['means'][:, k], label=f'Component {k+1}')\n",
    "\n",
    "for k in range(n_components):\n",
    "    plt.axhline(true_means[k], color='r', linestyle='--')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean')\n",
    "plt.title('Component Means Over Iterations (Check for Label Switching)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# One approach to handle label switching is to sort the components at each iteration\n",
    "# Let's reprocess the samples with sorted components\n",
    "\n",
    "# Initialize storage for sorted samples\n",
    "sorted_samples = {\n",
    "    'means': np.zeros_like(mixture_samples['means']),\n",
    "    'stds': np.zeros_like(mixture_samples['stds']),\n",
    "    'weights': np.zeros_like(mixture_samples['weights'])\n",
    "}\n",
    "\n",
    "# Sort components by mean at each iteration\n",
    "for i in range(len(mixture_samples['means'])):\n",
    "    sort_idx = np.argsort(mixture_samples['means'][i])\n",
    "    sorted_samples['means'][i] = mixture_samples['means'][i, sort_idx]\n",
    "    sorted_samples['stds'][i] = mixture_samples['stds'][i, sort_idx]\n",
    "    sorted_samples['weights'][i] = mixture_samples['weights'][i, sort_idx]\n",
    "\n",
    "# Plot sorted means\n",
    "plt.figure(figsize=(10, 6))\n",
    "for k in range(n_components):\n",
    "    plt.plot(sorted_samples['means'][:, k], label=f'Component {k+1}')\n",
    "\n",
    "# Sort true means for comparison\n",
    "true_means_sorted = np.sort(true_means)\n",
    "for k in range(n_components):\n",
    "    plt.axhline(true_means_sorted[k], color='r', linestyle='--')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean')\n",
    "plt.title('Sorted Component Means Over Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Limitations and Extensions of Gibbs Sampling\n",
    "\n",
    "### 5.1 Limitations\n",
    "\n",
    "Gibbs sampling, while powerful, has several limitations:\n",
    "\n",
    "1. **Requires Conditional Distributions**: The algorithm requires that we can sample from the conditional distribution of each variable given all others, which may not always be possible or efficient.\n",
    "\n",
    "2. **Poor Mixing with Strong Correlations**: When variables are highly correlated, the coordinate-wise updates of Gibbs sampling can lead to slow mixing and exploration of the parameter space.\n",
    "\n",
    "3. **Difficulty with Complex Posteriors**: For complex posterior distributions with multiple modes or irregular shapes, Gibbs sampling may get stuck in one region.\n",
    "\n",
    "4. **Label Switching in Mixture Models**: As we saw, mixture models suffer from the label switching problem, where component identities can change during sampling.\n",
    "\n",
    "### 5.2 Extensions and Alternatives\n",
    "\n",
    "Several extensions and alternatives address these limitations:\n",
    "\n",
    "1. **Blocked Gibbs Sampling**: Updates groups of variables together, which can improve mixing when variables are correlated.\n",
    "\n",
    "2. **Collapsed Gibbs Sampling**: Integrates out some variables analytically, reducing the dimensionality of the sampling space.\n",
    "\n",
    "3. **Metropolis-within-Gibbs**: Uses Metropolis steps for variables whose conditionals are difficult to sample from directly.\n",
    "\n",
    "4. **Hamiltonian Monte Carlo (HMC)**: Uses gradient information to propose more distant states, reducing random walk behavior and improving mixing in high dimensions.\n",
    "\n",
    "5. **No-U-Turn Sampler (NUTS)**: An adaptive version of HMC that automatically tunes the number of leapfrog steps.\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "In this notebook, we've explored the Gibbs sampling algorithm, a powerful MCMC method for sampling from multivariate distributions. We've implemented it from scratch and applied it to sample from bivariate normal distributions and to fit a Bayesian mixture model.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. Gibbs sampling is particularly effective when the conditional distributions are known and easy to sample from.\n",
    "\n",
    "2. The algorithm updates one variable at a time, which can be inefficient when variables are highly correlated.\n",
    "\n",
    "3. For problems with a natural conditional structure, such as hierarchical models and mixture models, Gibbs sampling can be very efficient.\n",
    "\n",
    "4. The performance of Gibbs sampling depends on the correlation structure of the target distribution and the parameterization of the model.\n",
    "\n",
    "5. Extensions like blocked Gibbs sampling and collapsed Gibbs sampling can improve efficiency for specific problems.\n",
    "\n",
    "In the next notebook, we'll explore Hamiltonian Monte Carlo (HMC), which addresses some of the limitations of Gibbs sampling, particularly for high-dimensional problems with complex geometries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
