{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Bayesian Inference: Analytical Approach vs MCMC with PyMC\n",
    "\n",
    "This notebook compares two approaches for solving Bayesian inference problems:\n",
    "1. Analytical approach using conjugate distributions\n",
    "2. MCMC approach using PyMC\n",
    "\n",
    "We will see how both methods can be used to solve the same problem and analyze their differences, advantages, and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration for plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# Configuration for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Estimating the success rate in a binomial process\n",
    "\n",
    "Let's consider a classic problem in Bayesian inference: estimating the probability of success $\\theta$ in a binomial process (for example, the probability of a coin landing heads).\n",
    "\n",
    "### Problem Context\n",
    "\n",
    "Imagine we have a coin that might be biased. We want to estimate the probability $\\theta$ that the coin lands heads. To do this, we conduct an experiment by flipping the coin several times and recording the results.\n",
    "\n",
    "In this case, we'll use simulated data for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameter (unknown in a real case)\n",
    "theta_true = 0.7\n",
    "\n",
    "# Number of flips\n",
    "n_trials = 50\n",
    "\n",
    "# Generate simulated data (1 = heads, 0 = tails)\n",
    "data = np.random.binomial(1, theta_true, size=n_trials)\n",
    "\n",
    "# Number of successes (heads)\n",
    "n_success = np.sum(data)\n",
    "\n",
    "print(f\"Number of flips: {n_trials}\")\n",
    "print(f\"Number of heads: {n_success}\")\n",
    "print(f\"Proportion of heads: {n_success/n_trials:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analytical Approach: Conjugate Distributions\n",
    "\n",
    "In Bayesian inference, when the prior distribution and likelihood have mathematical forms that allow a closed-form solution for the posterior distribution, we talk about conjugate distributions.\n",
    "\n",
    "For the case of a binomial distribution, the natural conjugate prior is the Beta distribution. If we use a Beta distribution as a prior, the posterior will also be a Beta distribution.\n",
    "\n",
    "### Prior Distribution\n",
    "\n",
    "We'll use a Beta distribution as a prior for $\\theta$:\n",
    "\n",
    "$$\\theta \\sim \\text{Beta}(\\alpha, \\beta)$$\n",
    "\n",
    "where $\\alpha$ and $\\beta$ are the parameters of the Beta distribution. For this example, we'll use $\\alpha = 1$ and $\\beta = 1$, which corresponds to a uniform distribution on $[0, 1]$, representing that we have no prior information about the bias of the coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the prior distribution\n",
    "alpha_prior = 1\n",
    "beta_prior = 1\n",
    "\n",
    "# Visualize the prior distribution\n",
    "theta_values = np.linspace(0, 1, 1000)\n",
    "prior_pdf = stats.beta.pdf(theta_values, alpha_prior, beta_prior)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(theta_values, prior_pdf, 'b-', lw=3, label='Prior distribution: Beta(1, 1)')\n",
    "plt.xlabel('θ (probability of heads)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Prior Distribution Beta(1, 1)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "\n",
    "The likelihood for a binomial process with $n$ trials and $k$ successes is:\n",
    "\n",
    "$$P(\\text{data}|\\theta) = \\binom{n}{k} \\theta^k (1-\\theta)^{n-k}$$\n",
    "\n",
    "Let's visualize this likelihood function for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood function\n",
    "def likelihood(theta, n, k):\n",
    "    return stats.binom.pmf(k, n, theta)\n",
    "\n",
    "# Visualize the likelihood\n",
    "likelihood_values = [likelihood(theta, n_trials, n_success) for theta in theta_values]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(theta_values, likelihood_values, 'r-', lw=3, label=f'Likelihood: Binomial({n_trials}, {n_success})')\n",
    "plt.xlabel('θ (probability of heads)')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.title(f'Likelihood function for {n_success} heads in {n_trials} flips')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Posterior Distribution\n",
    "\n",
    "For a Beta($\\alpha$, $\\beta$) prior distribution and a binomial likelihood with $n$ trials and $k$ successes, the posterior distribution is Beta($\\alpha + k$, $\\beta + n - k$).\n",
    "\n",
    "In our case, the posterior distribution will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the posterior distribution\n",
    "alpha_posterior = alpha_prior + n_success\n",
    "beta_posterior = beta_prior + n_trials - n_success\n",
    "\n",
    "print(f\"Posterior distribution: Beta({alpha_posterior}, {beta_posterior})\")\n",
    "\n",
    "# Visualize the posterior distribution\n",
    "posterior_pdf = stats.beta.pdf(theta_values, alpha_posterior, beta_posterior)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(theta_values, prior_pdf, 'b-', lw=2, label='Prior: Beta(1, 1)')\n",
    "plt.plot(theta_values, posterior_pdf, 'g-', lw=3, label=f'Posterior: Beta({alpha_posterior}, {beta_posterior})')\n",
    "plt.axvline(x=theta_true, color='r', linestyle='--', label=f'True value: θ = {theta_true}')\n",
    "plt.xlabel('θ (probability of heads)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Analytical Posterior Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics of the Posterior Distribution\n",
    "\n",
    "Let's calculate some statistics of interest from the posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior mean\n",
    "posterior_mean = alpha_posterior / (alpha_posterior + beta_posterior)\n",
    "print(f\"Posterior mean: {posterior_mean:.4f}\")\n",
    "\n",
    "# Posterior mode\n",
    "posterior_mode = (alpha_posterior - 1) / (alpha_posterior + beta_posterior - 2) if alpha_posterior > 1 and beta_posterior > 1 else \"N/A\"\n",
    "print(f\"Posterior mode: {posterior_mode:.4f}\")\n",
    "\n",
    "# 95% credible interval\n",
    "ci_lower, ci_upper = stats.beta.interval(0.95, alpha_posterior, beta_posterior)\n",
    "print(f\"95% credible interval: [{ci_lower:.4f}, {ci_upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Visualization of the Analytical Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(theta_values, prior_pdf, 'b-', lw=2, label='Prior: Beta(1, 1)')\n",
    "plt.plot(theta_values, likelihood_values / max(likelihood_values), 'r-', lw=2, \n",
    "         label=f'Likelihood (normalized): Binomial({n_trials}, {n_success})')\n",
    "plt.plot(theta_values, posterior_pdf, 'g-', lw=3, label=f'Posterior: Beta({alpha_posterior}, {beta_posterior})')\n",
    "plt.axvline(x=theta_true, color='k', linestyle='--', label=f'True value: θ = {theta_true}')\n",
    "plt.axvline(x=posterior_mean, color='g', linestyle=':', label=f'Posterior mean: {posterior_mean:.4f}')\n",
    "plt.axvspan(ci_lower, ci_upper, alpha=0.2, color='g', label=f'95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]')\n",
    "plt.xlabel('θ (probability of heads)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Analytical Bayesian Inference: Prior, Likelihood, and Posterior')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MCMC Approach with PyMC\n",
    "\n",
    "Now we'll solve the same problem using Markov Chain Monte Carlo (MCMC) methods implemented in PyMC. Although for this simple problem it's not necessary to use MCMC (since we have an analytical solution), this approach allows us to illustrate how MCMC methods work and how they are used in practice.\n",
    "\n",
    "### Model Definition in PyMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bayesian model in PyMC\n",
    "with pm.Model() as binomial_model:\n",
    "    # Prior\n",
    "    theta = pm.Beta('theta', alpha=alpha_prior, beta=beta_prior)\n",
    "    \n",
    "    # Likelihood\n",
    "    y = pm.Binomial('y', n=n_trials, p=theta, observed=n_success)\n",
    "    \n",
    "    # Sampling\n",
    "    trace = pm.sample(2000, tune=1000, return_inferencedata=True)\n",
    "\n",
    "# Summary of results\n",
    "summary = az.summary(trace)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of MCMC Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC chain trace\n",
    "az.plot_trace(trace)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior distribution of theta\n",
    "az.plot_posterior(trace, hdi_prob=0.95)\n",
    "plt.axvline(x=theta_true, color='r', linestyle='--', label=f'True value: θ = {theta_true}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of MCMC Posterior with Analytical Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract theta samples\n",
    "theta_samples = trace.posterior['theta'].values.flatten()\n",
    "\n",
    "# Compare posterior distributions\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Histogram of MCMC samples\n",
    "plt.hist(theta_samples, bins=30, density=True, alpha=0.5, label='MCMC Posterior (histogram)')\n",
    "\n",
    "# KDE density of MCMC samples\n",
    "sns.kdeplot(theta_samples, label='MCMC Posterior (KDE)', color='blue')\n",
    "\n",
    "# Analytical posterior distribution\n",
    "plt.plot(theta_values, posterior_pdf, 'r-', lw=3, label=f'Analytical Posterior: Beta({alpha_posterior}, {beta_posterior})')\n",
    "\n",
    "# True value and posterior mean\n",
    "plt.axvline(x=theta_true, color='k', linestyle='--', label=f'True value: θ = {theta_true}')\n",
    "plt.axvline(x=posterior_mean, color='g', linestyle=':', label=f'Analytical posterior mean: {posterior_mean:.4f}')\n",
    "plt.axvline(x=summary.loc['theta', 'mean'], color='b', linestyle=':', label=f'MCMC posterior mean: {summary.loc[\"theta\", \"mean\"]:.4f}')\n",
    "\n",
    "# Analytical credible interval\n",
    "plt.axvspan(ci_lower, ci_upper, alpha=0.2, color='r', label=f'Analytical 95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]')\n",
    "\n",
    "# MCMC HDI interval\n",
    "hdi_lower = summary.loc['theta', 'hdi_3%']\n",
    "hdi_upper = summary.loc['theta', 'hdi_97%']\n",
    "plt.axvspan(hdi_lower, hdi_upper, alpha=0.2, color='b', label=f'MCMC 95% HDI: [{hdi_lower:.4f}, {hdi_upper:.4f}]')\n",
    "\n",
    "plt.xlabel('θ (probability of heads)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Comparison of Posterior Distributions: Analytical vs MCMC')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MCMC Diagnostics\n",
    "\n",
    "One advantage of MCMC methods is that they provide diagnostic tools to evaluate the convergence and quality of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation plot\n",
    "az.plot_autocorr(trace)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy plot\n",
    "az.plot_energy(trace)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic statistics\n",
    "az.plot_forest(trace, var_names=['theta'], combined=True, kind='ridgeplot')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison of Approaches: Analytical vs MCMC\n",
    "\n",
    "Now that we've solved the same problem using two different approaches, we can compare their results and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to compare the results\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Analytical', 'MCMC'],\n",
    "    'Mean': [posterior_mean, summary.loc['theta', 'mean']],\n",
    "    'Standard deviation': [stats.beta.std(alpha_posterior, beta_posterior), summary.loc['theta', 'sd']],\n",
    "    'Lower interval': [ci_lower, hdi_lower],\n",
    "    'Upper interval': [ci_upper, hdi_upper]\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences Between Approaches\n",
    "\n",
    "#### Analytical Approach\n",
    "- **Advantages**:\n",
    "  - Exact and deterministic solution\n",
    "  - Computationally efficient\n",
    "  - No convergence diagnostics required\n",
    "- **Disadvantages**:\n",
    "  - Only applicable to certain models with conjugate distributions\n",
    "  - Limited to relatively simple models\n",
    "  - Difficult to extend to complex hierarchical models\n",
    "\n",
    "#### MCMC Approach\n",
    "- **Advantages**:\n",
    "  - Applicable to almost any Bayesian model\n",
    "  - Can handle complex and high-dimensional models\n",
    "  - Provides direct samples from the posterior distribution\n",
    "  - Facilitates the calculation of any posterior statistic of interest\n",
    "- **Disadvantages**:\n",
    "  - Stochastic approximation (not exact)\n",
    "  - Computationally more intensive\n",
    "  - Requires convergence diagnostics\n",
    "  - May suffer from mixing and convergence problems in complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. More Complex Example: Bayesian Linear Regression\n",
    "\n",
    "To better illustrate the advantages of MCMC, let's look at a slightly more complex example: Bayesian linear regression. In this case, the analytical solution is possible but more complicated, while the MCMC approach remains relatively simple to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for linear regression\n",
    "n_points = 30\n",
    "x = np.linspace(0, 10, n_points)\n",
    "true_intercept = 2.5\n",
    "true_slope = 1.8\n",
    "true_sigma = 2.0\n",
    "\n",
    "# y = intercept + slope * x + error\n",
    "y = true_intercept + true_slope * x + np.random.normal(0, true_sigma, size=n_points)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, color='blue', s=50, alpha=0.8, label='Observed data')\n",
    "plt.plot(x, true_intercept + true_slope * x, 'r-', lw=2, label='True line')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Data for Linear Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Linear Regression with PyMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian linear regression model with PyMC\n",
    "with pm.Model() as linear_model:\n",
    "    # Priors\n",
    "    intercept = pm.Normal('intercept', mu=0, sigma=10)\n",
    "    slope = pm.Normal('slope', mu=0, sigma=10)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=10)\n",
    "    \n",
    "    # Expected value of the distribution\n",
    "    mu = intercept + slope * x\n",
    "    \n",
    "    # Likelihood\n",
    "    likelihood = pm.Normal('likelihood', mu=mu, sigma=sigma, observed=y)\n",
    "    \n",
    "    # Sampling\n",
    "    trace_lm = pm.sample(2000, tune=1000, return_inferencedata=True)\n",
    "\n",
    "# Summary of results\n",
    "summary_lm = az.summary(trace_lm)\n",
    "summary_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the traces\n",
    "az.plot_trace(trace_lm)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the posterior distributions\n",
    "az.plot_posterior(trace_lm, var_names=['intercept', 'slope', 'sigma'], hdi_prob=0.95)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples\n",
    "intercept_samples = trace_lm.posterior['intercept'].values.flatten()\n",
    "slope_samples = trace_lm.posterior['slope'].values.flatten()\n",
    "sigma_samples = trace_lm.posterior['sigma'].values.flatten()\n",
    "\n",
    "# Visualize the regression with credible intervals\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Original data\n",
    "plt.scatter(x, y, color='blue', s=50, alpha=0.8, label='Observed data')\n",
    "\n",
    "# True line\n",
    "plt.plot(x, true_intercept + true_slope * x, 'r-', lw=2, label='True line')\n",
    "\n",
    "# Estimated line (posterior mean)\n",
    "intercept_mean = summary_lm.loc['intercept', 'mean']\n",
    "slope_mean = summary_lm.loc['slope', 'mean']\n",
    "plt.plot(x, intercept_mean + slope_mean * x, 'g-', lw=2, label='Estimated line (posterior mean)')\n",
    "\n",
    "# Credible interval for the line\n",
    "x_pred = np.linspace(0, 10, 100)\n",
    "y_pred_samples = np.array([intercept + slope * x_pred for intercept, slope in zip(intercept_samples[:500], slope_samples[:500])])\n",
    "y_pred_mean = y_pred_samples.mean(axis=0)\n",
    "y_pred_lower = np.percentile(y_pred_samples, 2.5, axis=0)\n",
    "y_pred_upper = np.percentile(y_pred_samples, 97.5, axis=0)\n",
    "\n",
    "plt.fill_between(x_pred, y_pred_lower, y_pred_upper, color='g', alpha=0.2, label='95% CI for the line')\n",
    "\n",
    "# Prediction interval (includes sigma uncertainty)\n",
    "y_pred_samples_with_noise = np.array([intercept + slope * x_pred + np.random.normal(0, sigma, size=len(x_pred)) \n",
    "                                     for intercept, slope, sigma in zip(intercept_samples[:500], slope_samples[:500], sigma_samples[:500])])\n",
    "y_pred_lower_with_noise = np.percentile(y_pred_samples_with_noise, 2.5, axis=0)\n",
    "y_pred_upper_with_noise = np.percentile(y_pred_samples_with_noise, 97.5, axis=0)\n",
    "\n",
    "plt.fill_between(x_pred, y_pred_lower_with_noise, y_pred_upper_with_noise, color='b', alpha=0.1, label='95% Prediction interval')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Bayesian Linear Regression with Credible Intervals')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions\n",
    "\n",
    "In this notebook, we've compared two approaches for Bayesian inference:\n",
    "\n",
    "1. **Analytical approach with conjugate distributions**:\n",
    "   - Provides exact and efficient solutions\n",
    "   - Limited to certain models with conjugate distributions\n",
    "   - Ideal for simple and well-defined problems\n",
    "\n",
    "2. **MCMC approach with PyMC**:\n",
    "   - Provides numerical approximations through samples\n",
    "   - Applicable to a wide range of complex models\n",
    "   - Requires convergence diagnostics\n",
    "   - Ideal for complex problems where analytical solutions are not feasible\n",
    "\n",
    "For simple problems like estimating the probability in a binomial process, both approaches produce very similar results. However, as models become more complex, the MCMC approach becomes increasingly valuable, as illustrated in the Bayesian linear regression example.\n",
    "\n",
    "In practice, it's important to know both approaches and choose the most appropriate one based on the complexity of the problem, the availability of conjugate distributions, and computational requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
