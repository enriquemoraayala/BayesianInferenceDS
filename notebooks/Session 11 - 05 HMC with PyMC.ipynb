{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamiltonian Monte Carlo (HMC) with PyMC\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll implement the same examples from the Hamiltonian Monte Carlo (HMC) algorithm notebook using PyMC, a probabilistic programming library in Python. PyMC provides a high-level interface for Bayesian modeling and includes efficient implementations of various MCMC algorithms, including HMC and its adaptive variant, the No-U-Turn Sampler (NUTS).\n",
    "\n",
    "We'll focus on:\n",
    "1. Sampling from a multivariate normal distribution\n",
    "2. Bayesian logistic regression\n",
    "\n",
    "Let's begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sampling from a Multivariate Normal Distribution\n",
    "\n",
    "First, let's recreate the multivariate normal example using PyMC. We'll sample from a 2D normal distribution with mean $\\mu = [0, 0]$ and covariance matrix $\\Sigma = \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}$, where $\\rho$ is the correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define parameters for the target distribution\n",
    "mean = np.array([0, 0])\n",
    "rho = 0.95  # High correlation to demonstrate HMC's advantage\n",
    "cov = np.array([[1, rho], [rho, 1]])\n",
    "\n",
    "# Create PyMC model\n",
    "with pm.Model() as mvn_model:\n",
    "    # Define multivariate normal distribution\n",
    "    x = pm.MvNormal('x', mu=mean, cov=cov)\n",
    "    \n",
    "    # Use HMC sampler\n",
    "    step = pm.HamiltonianMC()\n",
    "    \n",
    "    # Sample from the posterior\n",
    "    trace_hmc = pm.sample(5000, step=step, tune=1000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the results using ArviZ, a library for exploratory analysis of Bayesian models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot trace and posterior distributions\n",
    "az.plot_trace(trace_hmc)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot joint distribution\n",
    "az.plot_pair(trace_hmc, var_names=['x'], kind='scatter', divergences=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract samples for visualization\n",
    "pymc_samples = np.vstack([trace_hmc.posterior.x[:, :, 0].values.flatten(), \n",
    "                          trace_hmc.posterior.x[:, :, 1].values.flatten()]).T\n",
    "\n",
    "# Plot the samples\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(pymc_samples[:, 0], pymc_samples[:, 1], alpha=0.5, s=5)\n",
    "\n",
    "# Create a grid of points for contour plot\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pos = np.dstack((X, Y))\n",
    "\n",
    "# Compute PDF values on the grid\n",
    "rv = stats.multivariate_normal(mean, cov)\n",
    "Z = rv.pdf(pos)\n",
    "\n",
    "# Plot contours\n",
    "levels = np.linspace(0, Z.max(), 10)[1:]\n",
    "plt.contour(X, Y, Z, levels=levels, colors='r', alpha=0.7)\n",
    "\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title(f\"PyMC HMC Samples (ρ={rho})\")\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Comparison with NUTS\n",
    "\n",
    "PyMC's default sampler is NUTS (No-U-Turn Sampler), which is an adaptive extension of HMC that automatically tunes the number of leapfrog steps. Let's compare the performance of HMC and NUTS for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create PyMC model with NUTS\n",
    "with pm.Model() as mvn_model_nuts:\n",
    "    # Define multivariate normal distribution\n",
    "    x = pm.MvNormal('x', mu=mean, cov=cov)\n",
    "    \n",
    "    # Use NUTS sampler (default)\n",
    "    trace_nuts = pm.sample(5000, tune=1000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot trace and posterior distributions\n",
    "az.plot_trace(trace_nuts)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare autocorrelation between HMC and NUTS\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Calculate autocorrelation for HMC\n",
    "hmc_samples = trace_hmc.posterior.x[:, :, 0].values.flatten()\n",
    "max_lag = 50\n",
    "hmc_acf = np.zeros(max_lag)\n",
    "for lag in range(max_lag):\n",
    "    hmc_acf[lag] = np.corrcoef(hmc_samples[lag:], hmc_samples[:-lag if lag > 0 else None])[0, 1]\n",
    "\n",
    "# Calculate autocorrelation for NUTS\n",
    "nuts_samples = trace_nuts.posterior.x[:, :, 0].values.flatten()\n",
    "nuts_acf = np.zeros(max_lag)\n",
    "for lag in range(max_lag):\n",
    "    nuts_acf[lag] = np.corrcoef(nuts_samples[lag:], nuts_samples[:-lag if lag > 0 else None])[0, 1]\n",
    "\n",
    "plt.plot(range(max_lag), hmc_acf, label='HMC')\n",
    "plt.plot(range(max_lag), nuts_acf, label='NUTS')\n",
    "plt.title('Autocorrelation Comparison: HMC vs NUTS')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare effective sample size (ESS) between HMC and NUTS\n",
    "ess_hmc = az.ess(trace_hmc, var_names=['x'])\n",
    "ess_nuts = az.ess(trace_nuts, var_names=['x'])\n",
    "\n",
    "# Calculate ESS per sample\n",
    "n_samples_hmc = len(trace_hmc.posterior.draw)\n",
    "n_samples_nuts = len(trace_nuts.posterior.draw)\n",
    "\n",
    "ess_per_sample_hmc = ess_hmc.x.mean().item() / n_samples_hmc\n",
    "ess_per_sample_nuts = ess_nuts.x.mean().item() / n_samples_nuts\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Sampler': ['HMC', 'NUTS'],\n",
    "    'ESS': [ess_hmc.x.mean().item(), ess_nuts.x.mean().item()],\n",
    "    'ESS per Sample': [ess_per_sample_hmc, ess_per_sample_nuts]\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(comparison_df['Sampler'], comparison_df['ESS per Sample'])\n",
    "plt.title('Effective Sample Size per Sample: HMC vs NUTS')\n",
    "plt.ylabel('ESS per Sample')\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Comparison with Metropolis and Gibbs Sampling\n",
    "\n",
    "Let's compare the performance of HMC and NUTS with the Metropolis and Gibbs sampling algorithms for the same multivariate normal distribution with high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create PyMC model with Metropolis\n",
    "with pm.Model() as mvn_model_metropolis:\n",
    "    # Define multivariate normal distribution\n",
    "    x = pm.MvNormal('x', mu=mean, cov=cov)\n",
    "    \n",
    "    # Use Metropolis sampler\n",
    "    step = pm.Metropolis()\n",
    "    trace_metropolis = pm.sample(5000, step=step, tune=1000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create PyMC model with Gibbs-like approach\n",
    "with pm.Model() as mvn_model_gibbs:\n",
    "    # Define x1 and x2 as separate variables\n",
    "    x1 = pm.Normal('x1', mu=0, sigma=1)\n",
    "    \n",
    "    # Define x2 conditionally on x1\n",
    "    # For a bivariate normal with correlation rho, the conditional distribution is:\n",
    "    # x2|x1 ~ N(rho*x1, sqrt(1-rho^2))\n",
    "    x2 = pm.Normal('x2', mu=rho * x1, sigma=np.sqrt(1 - rho**2))\n",
    "    \n",
    "    # Create a compound step method that updates each variable separately\n",
    "    step1 = pm.Metropolis(vars=[x1])\n",
    "    step2 = pm.Metropolis(vars=[x2])\n",
    "    \n",
    "    # Sample using the compound step method\n",
    "    trace_gibbs = pm.sample(5000, step=[step1, step2], tune=1000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare effective sample size (ESS) across all methods\n",
    "ess_metropolis = az.ess(trace_metropolis, var_names=['x'])\n",
    "ess_gibbs_x1 = az.ess(trace_gibbs, var_names=['x1'])\n",
    "ess_gibbs_x2 = az.ess(trace_gibbs, var_names=['x2'])\n",
    "ess_gibbs_avg = (ess_gibbs_x1.x1.item() + ess_gibbs_x2.x2.item()) / 2\n",
    "\n",
    "# Calculate ESS per sample\n",
    "n_samples_metropolis = len(trace_metropolis.posterior.draw)\n",
    "n_samples_gibbs = len(trace_gibbs.posterior.draw)\n",
    "\n",
    "ess_per_sample_metropolis = ess_metropolis.x.mean().item() / n_samples_metropolis\n",
    "ess_per_sample_gibbs = ess_gibbs_avg / n_samples_gibbs\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Sampler': ['Metropolis', 'Gibbs-like', 'HMC', 'NUTS'],\n",
    "    'ESS': [ess_metropolis.x.mean().item(), ess_gibbs_avg, ess_hmc.x.mean().item(), ess_nuts.x.mean().item()],\n",
    "    'ESS per Sample': [ess_per_sample_metropolis, ess_per_sample_gibbs, ess_per_sample_hmc, ess_per_sample_nuts]\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(comparison_df['Sampler'], comparison_df['ESS per Sample'])\n",
    "plt.title(f'Effective Sample Size per Sample Across Methods (ρ={rho})')\n",
    "plt.ylabel('ESS per Sample')\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot samples from each method\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Extract samples\n",
    "metropolis_samples = np.vstack([trace_metropolis.posterior.x[:, :, 0].values.flatten(), \n",
    "                               trace_metropolis.posterior.x[:, :, 1].values.flatten()]).T\n",
    "\n",
    "gibbs_samples = np.vstack([trace_gibbs.posterior.x1.values.flatten(), \n",
    "                          trace_gibbs.posterior.x2.values.flatten()]).T\n",
    "\n",
    "hmc_samples = np.vstack([trace_hmc.posterior.x[:, :, 0].values.flatten(), \n",
    "                        trace_hmc.posterior.x[:, :, 1].values.flatten()]).T\n",
    "\n",
    "nuts_samples = np.vstack([trace_nuts.posterior.x[:, :, 0].values.flatten(), \n",
    "                         trace_nuts.posterior.x[:, :, 1].values.flatten()]).T\n",
    "\n",
    "# Plot samples\n",
    "axes[0, 0].scatter(metropolis_samples[:, 0], metropolis_samples[:, 1], alpha=0.5, s=5)\n",
    "axes[0, 0].set_title(f\"Metropolis (ESS/N={ess_per_sample_metropolis:.3f})\")\n",
    "axes[0, 0].set_xlim(-3, 3)\n",
    "axes[0, 0].set_ylim(-3, 3)\n",
    "\n",
    "axes[0, 1].scatter(gibbs_samples[:, 0], gibbs_samples[:, 1], alpha=0.5, s=5)\n",
    "axes[0, 1].set_title(f\"Gibbs-like (ESS/N={ess_per_sample_gibbs:.3f})\")\n",
    "axes[0, 1].set_xlim(-3, 3)\n",
    "axes[0, 1].set_ylim(-3, 3)\n",
    "\n",
    "axes[1, 0].scatter(hmc_samples[:, 0], hmc_samples[:, 1], alpha=0.5, s=5)\n",
    "axes[1, 0].set_title(f\"HMC (ESS/N={ess_per_sample_hmc:.3f})\")\n",
    "axes[1, 0].set_xlim(-3, 3)\n",
    "axes[1, 0].set_ylim(-3, 3)\n",
    "\n",
    "axes[1, 1].scatter(nuts_samples[:, 0], nuts_samples[:, 1], alpha=0.5, s=5)\n",
    "axes[1, 1].set_title(f\"NUTS (ESS/N={ess_per_sample_nuts:.3f})\")\n",
    "axes[1, 1].set_xlim(-3, 3)\n",
    "axes[1, 1].set_ylim(-3, 3)\n",
    "\n",
    "# Plot contours of the true distribution on all subplots\n",
    "for ax in axes.flatten():\n",
    "    ax.contour(X, Y, Z, levels=levels, colors='r', alpha=0.7)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayesian Logistic Regression\n",
    "\n",
    "Now, let's implement the Bayesian logistic regression example using PyMC. We'll use the same synthetic data as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set true parameter values\n",
    "true_beta = np.array([0.5, 2.0, -1.0])  # Intercept and coefficients\n",
    "n_features = len(true_beta) - 1\n",
    "\n",
    "# Generate synthetic data\n",
    "n_data = 200\n",
    "X = np.random.normal(0, 1, size=(n_data, n_features))\n",
    "X_with_intercept = np.column_stack([np.ones(n_data), X])\n",
    "logits = X_with_intercept @ true_beta\n",
    "p = 1 / (1 + np.exp(-logits))\n",
    "y = np.random.binomial(1, p)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Synthetic Data for Bayesian Logistic Regression')\n",
    "plt.colorbar(label='Class')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 PyMC Implementation\n",
    "\n",
    "Let's implement the Bayesian logistic regression model using PyMC. We'll use the same prior as before: $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, 10^2 \\mathbf{I})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create PyMC model for logistic regression\n",
    "with pm.Model() as logistic_model:\n",
    "    # Priors for unknown model parameters\n",
    "    beta = pm.Normal('beta', mu=0, sigma=10, shape=len(true_beta))\n",
    "    \n",
    "    # Expected value of outcome\n",
    "    mu = pm.math.dot(X_with_intercept, beta)\n",
    "    \n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    y_obs = pm.Bernoulli('y_obs', p=pm.math.sigmoid(mu), observed=y)\n",
    "    \n",
    "    # Use HMC sampler\n",
    "    step = pm.HamiltonianMC()\n",
    "    trace_logistic_hmc = pm.sample(5000, step=step, tune=1000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot trace and posterior distributions\n",
    "az.plot_trace(trace_logistic_hmc)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot posterior distributions with true values\n",
    "az.plot_posterior(trace_logistic_hmc, var_names=['beta'], ref_val=true_beta)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Summarize posterior statistics\n",
    "summary = az.summary(trace_logistic_hmc, var_names=['beta'])\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Comparison with NUTS\n",
    "\n",
    "Let's compare the performance of HMC with NUTS for the logistic regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create PyMC model for logistic regression with NUTS\n",
    "with pm.Model() as logistic_model_nuts:\n",
    "    # Priors for unknown model parameters\n",
    "    beta = pm.Normal('beta', mu=0, sigma=10, shape=len(true_beta))\n",
    "    \n",
    "    # Expected value of outcome\n",
    "    mu = pm.math.dot(X_with_intercept, beta)\n",
    "    \n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    y_obs = pm.Bernoulli('y_obs', p=pm.math.sigmoid(mu), observed=y)\n",
    "    \n",
    "    # Use NUTS sampler (default)\n",
    "    trace_logistic_nuts = pm.sample(5000, tune=1000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare traces between HMC and NUTS\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot HMC traces\n",
    "for i in range(3):\n",
    "    samples = trace_logistic_hmc.posterior.beta[:, :, i].values.flatten()\n",
    "    axes[i, 0].plot(samples)\n",
    "    axes[i, 0].set_title(f\"beta[{i}] (HMC)\")\n",
    "    axes[i, 0].axhline(y=true_beta[i], color='r', linestyle='--')\n",
    "\n",
    "# Plot NUTS traces\n",
    "for i in range(3):\n",
    "    samples = trace_logistic_nuts.posterior.beta[:, :, i].values.flatten()\n",
    "    axes[i, 1].plot(samples)\n",
    "    axes[i, 1].set_title(f\"beta[{i}] (NUTS)\")\n",
    "    axes[i, 1].axhline(y=true_beta[i], color='r', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare effective sample size (ESS) between HMC and NUTS\n",
    "ess_logistic_hmc = az.ess(trace_logistic_hmc, var_names=['beta'])\n",
    "ess_logistic_nuts = az.ess(trace_logistic_nuts, var_names=['beta'])\n",
    "\n",
    "# Calculate ESS per sample\n",
    "n_samples_logistic_hmc = len(trace_logistic_hmc.posterior.draw)\n",
    "n_samples_logistic_nuts = len(trace_logistic_nuts.posterior.draw)\n",
    "\n",
    "ess_per_sample_logistic_hmc = ess_logistic_hmc.beta.mean().item() / n_samples_logistic_hmc\n",
    "ess_per_sample_logistic_nuts = ess_logistic_nuts.beta.mean().item() / n_samples_logistic_nuts\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Sampler': ['HMC', 'NUTS'],\n",
    "    'ESS': [ess_logistic_hmc.beta.mean().item(), ess_logistic_nuts.beta.mean().item()],\n",
    "    'ESS per Sample': [ess_per_sample_logistic_hmc, ess_per_sample_logistic_nuts]\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(comparison_df['Sampler'], comparison_df['ESS per Sample'])\n",
    "plt.title('Effective Sample Size per Sample: HMC vs NUTS for Logistic Regression')\n",
    "plt.ylabel('ESS per Sample')\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualizing the Decision Boundary\n",
    "\n",
    "Let's visualize the decision boundary and posterior uncertainty using the NUTS samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract posterior samples\n",
    "beta_samples = trace_logistic_nuts.posterior.beta.values.reshape(-1, 3)\n",
    "\n",
    "# Compute posterior mean\n",
    "beta_mean = beta_samples.mean(axis=0)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', alpha=0.7)\n",
    "\n",
    "# Create a grid of points\n",
    "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100), np.linspace(x2_min, x2_max, 100))\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "# Compute predicted probabilities for each point in the grid\n",
    "grid_with_intercept = np.column_stack([np.ones(grid.shape[0]), grid])\n",
    "\n",
    "# Use posterior mean for prediction\n",
    "logits = grid_with_intercept @ beta_mean\n",
    "probs = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "# Reshape probabilities to grid shape\n",
    "probs = probs.reshape(xx1.shape)\n",
    "\n",
    "# Plot decision boundary (p=0.5) and probability contours\n",
    "plt.contour(xx1, xx2, probs, levels=[0.5], colors='k', linestyles='--')\n",
    "plt.contourf(xx1, xx2, probs, alpha=0.3, cmap='coolwarm')\n",
    "\n",
    "# Plot true decision boundary\n",
    "# For logistic regression, the decision boundary is a line: beta[0] + beta[1]*x1 + beta[2]*x2 = 0\n",
    "# Solving for x2: x2 = -(beta[0] + beta[1]*x1) / beta[2]\n",
    "x1_line = np.array([x1_min, x1_max])\n",
    "x2_line_true = -(true_beta[0] + true_beta[1] * x1_line) / true_beta[2]\n",
    "plt.plot(x1_line, x2_line_true, 'r-', label='True boundary')\n",
    "\n",
    "# Plot posterior mean decision boundary\n",
    "x2_line_mean = -(beta_mean[0] + beta_mean[1] * x1_line) / beta_mean[2]\n",
    "plt.plot(x1_line, x2_line_mean, 'g-', label='Posterior mean boundary')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Bayesian Logistic Regression: Decision Boundary (PyMC)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot posterior uncertainty in decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', alpha=0.7)\n",
    "\n",
    "# Plot a sample of decision boundaries from the posterior\n",
    "n_samples_to_plot = 100\n",
    "sample_indices = np.random.choice(len(beta_samples), n_samples_to_plot, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    beta_sample = beta_samples[idx]\n",
    "    x2_line_sample = -(beta_sample[0] + beta_sample[1] * x1_line) / beta_sample[2]\n",
    "    plt.plot(x1_line, x2_line_sample, 'b-', alpha=0.05)\n",
    "\n",
    "# Plot true decision boundary\n",
    "plt.plot(x1_line, x2_line_true, 'r-', linewidth=2, label='True boundary')\n",
    "\n",
    "# Plot posterior mean decision boundary\n",
    "plt.plot(x1_line, x2_line_mean, 'g-', linewidth=2, label='Posterior mean boundary')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Bayesian Logistic Regression: Posterior Uncertainty in Decision Boundary (PyMC)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Posterior Predictive Checks\n",
    "\n",
    "Let's perform posterior predictive checks to assess the fit of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate posterior predictive samples\n",
    "with logistic_model_nuts:\n",
    "    posterior_pred = pm.sample_posterior_predictive(trace_logistic_nuts)\n",
    "\n",
    "# Convert to InferenceData\n",
    "posterior_pred_idata = az.from_pymc(posterior=trace_logistic_nuts, posterior_predictive=posterior_pred)\n",
    "\n",
    "# Plot posterior predictive check\n",
    "az.plot_ppc(posterior_pred_idata, group='posterior_predictive', data_pairs={\"y_obs\": y})\n",
    "plt.title('Posterior Predictive Check')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advantages of Using PyMC for HMC\n",
    "\n",
    "Based on our experiments, we can highlight several advantages of using PyMC for Hamiltonian Monte Carlo:\n",
    "\n",
    "1. **Efficient Implementation**: PyMC provides highly optimized implementations of HMC and NUTS, which can be much faster than custom implementations.\n",
    "\n",
    "2. **Automatic Differentiation**: PyMC uses automatic differentiation to compute gradients, eliminating the need to derive and implement gradients manually.\n",
    "\n",
    "3. **Adaptive Algorithms**: PyMC's NUTS algorithm automatically tunes the number of leapfrog steps, and both HMC and NUTS adaptively tune the step size during the warm-up phase.\n",
    "\n",
    "4. **Diagnostics and Visualization**: With ArviZ integration, PyMC provides comprehensive tools for diagnosing convergence issues and visualizing results.\n",
    "\n",
    "5. **High-Level Interface**: PyMC provides a high-level interface for defining complex probabilistic models, making it easy to apply HMC to a wide range of problems.\n",
    "\n",
    "6. **Handling of Edge Cases**: PyMC's implementation includes various optimizations and safeguards to handle numerical issues and edge cases.\n",
    "\n",
    "## 4. Conclusion\n",
    "\n",
    "In this notebook, we've implemented the same examples from the Hamiltonian Monte Carlo algorithm notebook using PyMC. We've seen how PyMC simplifies the process of defining models and sampling from them, while providing efficient implementations of HMC and its adaptive variant, NUTS.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. PyMC makes it easy to define and sample from probabilistic models using HMC and NUTS.\n",
    "\n",
    "2. NUTS generally provides better sampling efficiency than basic HMC, especially for complex models, by automatically tuning the number of leapfrog steps.\n",
    "\n",
    "3. Both HMC and NUTS significantly outperform simpler methods like Metropolis and Gibbs sampling for problems with correlated parameters or high dimensionality.\n",
    "\n",
    "4. PyMC's integration with ArviZ provides powerful tools for diagnosing and visualizing MCMC results.\n",
    "\n",
    "While implementing HMC from scratch is valuable for understanding its mechanics, using a library like PyMC is often more practical for real-world applications due to its efficiency, reliability, and ease of use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
