{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamiltonian Monte Carlo (HMC): Theory and Implementation from Scratch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Hamiltonian Monte Carlo (HMC), also known as Hybrid Monte Carlo, is an advanced Markov Chain Monte Carlo (MCMC) method that uses concepts from Hamiltonian dynamics to propose states that are distant from the current state while maintaining a high acceptance probability. This approach significantly reduces the random walk behavior that plagues simpler methods like Metropolis and Gibbs sampling, making HMC particularly effective for high-dimensional problems with complex geometries.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Explore the theoretical foundations of the Hamiltonian Monte Carlo algorithm\n",
    "2. Implement the algorithm from scratch in Python\n",
    "3. Apply it to sample from multivariate normal distributions\n",
    "4. Extend it to a more complex problem: Bayesian logistic regression\n",
    "5. Analyze the algorithm's performance and limitations\n",
    "\n",
    "Let's begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import autograd.numpy as anp\n",
    "from autograd import grad\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical Background\n",
    "\n",
    "### 1.1 The Hamiltonian Monte Carlo Algorithm\n",
    "\n",
    "Hamiltonian Monte Carlo (HMC) uses concepts from physics to propose states that are distant from the current state but still have a high probability of acceptance. It introduces auxiliary momentum variables and uses Hamiltonian dynamics to guide the proposals.\n",
    "\n",
    "For a target distribution $\\pi(\\mathbf{q}) \\propto \\exp(-U(\\mathbf{q}))$, where $U(\\mathbf{q})$ is the \"potential energy\" (negative log probability), the algorithm works as follows:\n",
    "\n",
    "1. Introduce momentum variables $\\mathbf{p}$ with distribution $\\pi(\\mathbf{p}) \\propto \\exp(-K(\\mathbf{p}))$, typically $K(\\mathbf{p}) = \\frac{1}{2}\\mathbf{p}^T\\mathbf{M}^{-1}\\mathbf{p}$ (multivariate normal with precision matrix $\\mathbf{M}^{-1}$)\n",
    "\n",
    "2. Define the Hamiltonian $H(\\mathbf{q}, \\mathbf{p}) = U(\\mathbf{q}) + K(\\mathbf{p})$\n",
    "\n",
    "3. For each iteration $t$:\n",
    "   a. Sample new momentum $\\mathbf{p} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{M})$\n",
    "   b. Simulate Hamiltonian dynamics for $L$ steps with step size $\\epsilon$:\n",
    "      - Leapfrog integration:\n",
    "        1. $\\mathbf{p}' = \\mathbf{p} - \\frac{\\epsilon}{2}\\nabla U(\\mathbf{q})$\n",
    "        2. $\\mathbf{q}' = \\mathbf{q} + \\epsilon\\mathbf{M}^{-1}\\mathbf{p}'$\n",
    "        3. $\\mathbf{p}' = \\mathbf{p}' - \\frac{\\epsilon}{2}\\nabla U(\\mathbf{q}')$\n",
    "   c. Calculate acceptance probability $\\alpha = \\min(1, \\exp(H(\\mathbf{q}, \\mathbf{p}) - H(\\mathbf{q}', \\mathbf{p}')))$\n",
    "   d. Accept or reject the proposal using Metropolis criterion\n",
    "\n",
    "### 1.2 Key Properties\n",
    "\n",
    "- **Gradient Information**: HMC uses the gradient of the log probability (potential energy) to guide the proposals, which helps it navigate the parameter space more efficiently.\n",
    "\n",
    "- **Hamiltonian Dynamics**: The Hamiltonian dynamics simulation allows the algorithm to propose states that are distant from the current state but still have a high probability of acceptance.\n",
    "\n",
    "- **Leapfrog Integration**: The leapfrog integrator is a symplectic integrator that preserves volume in phase space and is reversible, which is important for detailed balance.\n",
    "\n",
    "- **Tuning Parameters**: HMC has two main tuning parameters: the step size $\\epsilon$ and the number of leapfrog steps $L$. The step size affects the accuracy of the simulation, while the number of steps affects the distance of the proposals.\n",
    "\n",
    "### 1.3 Practical Considerations\n",
    "\n",
    "- **Gradient Computation**: HMC requires the gradient of the log probability, which can be computed analytically or using automatic differentiation.\n",
    "\n",
    "- **Mass Matrix**: The mass matrix $\\mathbf{M}$ can be adapted to the geometry of the target distribution. A common choice is the identity matrix, but using the inverse of the covariance matrix can improve efficiency.\n",
    "\n",
    "- **Step Size and Number of Steps**: These parameters need to be tuned for optimal performance. The step size should be small enough for accurate simulation but large enough for efficient exploration. The number of steps should be large enough to propose distant states but small enough to keep the computational cost reasonable.\n",
    "\n",
    "- **Adaptive HMC**: Extensions like the No-U-Turn Sampler (NUTS) automatically tune the number of leapfrog steps, while dual averaging can tune the step size.\n",
    "\n",
    "Now, let's implement the Hamiltonian Monte Carlo algorithm from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation from Scratch\n",
    "\n",
    "We'll start by implementing the leapfrog integrator, which is the core component of the HMC algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def leapfrog_integrator(q_current, p_current, grad_U, epsilon, L, M_inv=None):\n",
    "    \"\"\"\n",
    "    Leapfrog integrator for Hamiltonian dynamics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    q_current : array-like\n",
    "        Current position (parameter values)\n",
    "    p_current : array-like\n",
    "        Current momentum\n",
    "    grad_U : function\n",
    "        Function that computes the gradient of the potential energy (negative log probability)\n",
    "    epsilon : float\n",
    "        Step size for the integrator\n",
    "    L : int\n",
    "        Number of leapfrog steps\n",
    "    M_inv : array-like, optional\n",
    "        Inverse mass matrix. If None, the identity matrix is used.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    q_proposed : array-like\n",
    "        Proposed position after L leapfrog steps\n",
    "    p_proposed : array-like\n",
    "        Proposed momentum after L leapfrog steps\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    q = np.copy(q_current)\n",
    "    p = np.copy(p_current)\n",
    "    \n",
    "    # Set inverse mass matrix to identity if not provided\n",
    "    if M_inv is None:\n",
    "        M_inv = np.eye(len(q))\n",
    "    \n",
    "    # First half-step for momentum\n",
    "    p = p - 0.5 * epsilon * grad_U(q)\n",
    "    \n",
    "    # Full leapfrog steps\n",
    "    for i in range(L):\n",
    "        # Full step for position\n",
    "        q = q + epsilon * M_inv @ p\n",
    "        \n",
    "        # Full step for momentum, except at the end of the trajectory\n",
    "        if i < L - 1:\n",
    "            p = p - epsilon * grad_U(q)\n",
    "        else:\n",
    "            # Half-step for momentum at the end\n",
    "            p = p - 0.5 * epsilon * grad_U(q)\n",
    "    \n",
    "    return q, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement the full HMC algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def hamiltonian_monte_carlo(U, grad_U, initial_position, n_samples, epsilon, L, M=None, burn_in=0, thin=1):\n",
    "    \"\"\"\n",
    "    Hamiltonian Monte Carlo algorithm for sampling from a probability distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    U : function\n",
    "        Function that computes the potential energy (negative log probability)\n",
    "    grad_U : function\n",
    "        Function that computes the gradient of the potential energy\n",
    "    initial_position : array-like\n",
    "        Initial position (parameter values)\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    epsilon : float\n",
    "        Step size for the leapfrog integrator\n",
    "    L : int\n",
    "        Number of leapfrog steps\n",
    "    M : array-like, optional\n",
    "        Mass matrix. If None, the identity matrix is used.\n",
    "    burn_in : int, optional\n",
    "        Number of initial samples to discard\n",
    "    thin : int, optional\n",
    "        Thinning factor (keep every thin-th sample)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : array\n",
    "        Generated samples\n",
    "    acceptance_rate : float\n",
    "        Fraction of proposals that were accepted\n",
    "    \"\"\"\n",
    "    # Convert initial position to numpy array if it isn't already\n",
    "    q_current = np.array(initial_position, dtype=float)\n",
    "    dim = q_current.shape[0]  # Dimensionality of the position\n",
    "    \n",
    "    # Set mass matrix to identity if not provided\n",
    "    if M is None:\n",
    "        M = np.eye(dim)\n",
    "    \n",
    "    # Compute inverse mass matrix\n",
    "    M_inv = np.linalg.inv(M)\n",
    "    \n",
    "    # Total number of iterations needed\n",
    "    n_iterations = burn_in + thin * n_samples\n",
    "    \n",
    "    # Initialize storage for samples and acceptance tracking\n",
    "    samples = np.zeros((n_samples, dim))\n",
    "    n_accepted = 0\n",
    "    \n",
    "    # Main sampling loop\n",
    "    sample_idx = 0\n",
    "    for i in tqdm(range(n_iterations), desc=\"Sampling\"):\n",
    "        # Resample momentum\n",
    "        p_current = np.random.multivariate_normal(np.zeros(dim), M)\n",
    "        \n",
    "        # Compute Hamiltonian (energy) at current state\n",
    "        current_U = U(q_current)\n",
    "        current_K = 0.5 * p_current @ M_inv @ p_current\n",
    "        current_H = current_U + current_K\n",
    "        \n",
    "        # Simulate Hamiltonian dynamics using leapfrog integrator\n",
    "        q_proposed, p_proposed = leapfrog_integrator(q_current, p_current, grad_U, epsilon, L, M_inv)\n",
    "        \n",
    "        # Compute Hamiltonian (energy) at proposed state\n",
    "        proposed_U = U(q_proposed)\n",
    "        proposed_K = 0.5 * p_proposed @ M_inv @ p_proposed\n",
    "        proposed_H = proposed_U + proposed_K\n",
    "        \n",
    "        # Metropolis acceptance step\n",
    "        # Note: exp(current_H - proposed_H) = exp(-(proposed_H - current_H))\n",
    "        # = exp(-(proposed_U - current_U + proposed_K - current_K))\n",
    "        # = exp(-(proposed_U - current_U)) * exp(-(proposed_K - current_K))\n",
    "        # = (proposed_density / current_density) * (proposed_momentum_density / current_momentum_density)\n",
    "        delta_H = proposed_H - current_H\n",
    "        if np.log(np.random.random()) < -delta_H:  # Accept with probability min(1, exp(-delta_H))\n",
    "            q_current = q_proposed\n",
    "            n_accepted += 1\n",
    "        \n",
    "        # Store the sample if past burn-in and due for storage based on thinning\n",
    "        if i >= burn_in and (i - burn_in) % thin == 0:\n",
    "            samples[sample_idx] = q_current\n",
    "            sample_idx += 1\n",
    "    \n",
    "    # Compute acceptance rate\n",
    "    acceptance_rate = n_accepted / n_iterations\n",
    "    \n",
    "    return samples, acceptance_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example 1: Sampling from a Multivariate Normal Distribution\n",
    "\n",
    "Let's apply our HMC sampler to a multivariate normal distribution. This is a good test case because:\n",
    "1. We know the true distribution\n",
    "2. We can easily calculate the density and its gradient\n",
    "3. We can visualize the results in 2D\n",
    "4. It demonstrates how HMC can efficiently navigate correlated distributions\n",
    "\n",
    "We'll sample from a 2D normal distribution with mean $\\mu = [0, 0]$ and covariance matrix $\\Sigma = \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}$, where $\\rho$ is the correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define parameters for the target distribution\n",
    "mean = np.array([0, 0])\n",
    "rho = 0.95  # High correlation to demonstrate HMC's advantage\n",
    "cov = np.array([[1, rho], [rho, 1]])\n",
    "precision = np.linalg.inv(cov)  # Precision matrix (inverse covariance)\n",
    "\n",
    "# Define potential energy (negative log probability) and its gradient\n",
    "def U(q):\n",
    "    \"\"\"Potential energy (negative log probability) for multivariate normal.\"\"\"\n",
    "    delta = q - mean\n",
    "    return 0.5 * delta @ precision @ delta\n",
    "\n",
    "def grad_U(q):\n",
    "    \"\"\"Gradient of potential energy for multivariate normal.\"\"\"\n",
    "    delta = q - mean\n",
    "    return precision @ delta\n",
    "\n",
    "# Set sampling parameters\n",
    "initial_position = np.array([2, 2])  # Start away from the mean\n",
    "n_samples = 5000\n",
    "epsilon = 0.1  # Step size\n",
    "L = 10  # Number of leapfrog steps\n",
    "burn_in = 1000\n",
    "thin = 1\n",
    "\n",
    "# Run the HMC sampler\n",
    "samples, acceptance_rate = hamiltonian_monte_carlo(\n",
    "    U, grad_U, initial_position, n_samples, epsilon, L, burn_in=burn_in, thin=thin\n",
    ")\n",
    "\n",
    "print(f\"Acceptance rate: {acceptance_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the results to see how well our sampler approximates the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_2d_samples(samples, true_mean=None, true_cov=None, title=\"Samples\", show_trajectory=False, n_trajectory=100):\n",
    "    \"\"\"\n",
    "    Plot 2D samples with marginal distributions and optionally compare to true distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    samples : array-like\n",
    "        2D samples to plot\n",
    "    true_mean : array-like, optional\n",
    "        True mean vector for comparison\n",
    "    true_cov : array-like, optional\n",
    "        True covariance matrix for comparison\n",
    "    title : str, optional\n",
    "        Plot title\n",
    "    show_trajectory : bool, optional\n",
    "        Whether to show the trajectory of the first n_trajectory samples\n",
    "    n_trajectory : int, optional\n",
    "        Number of samples to include in trajectory plot\n",
    "    \"\"\"\n",
    "    # Create a figure with a grid for the joint and marginal plots\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    gs = fig.add_gridspec(2, 2, width_ratios=[3, 1], height_ratios=[1, 3],\n",
    "                         wspace=0.05, hspace=0.05)\n",
    "    \n",
    "    # Joint distribution plot\n",
    "    ax_joint = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    # Plot the samples\n",
    "    ax_joint.scatter(samples[:, 0], samples[:, 1], alpha=0.5, s=5)\n",
    "    \n",
    "    # If requested, show the trajectory of the first n_trajectory samples\n",
    "    if show_trajectory and n_trajectory > 0:\n",
    "        n = min(n_trajectory, len(samples))\n",
    "        ax_joint.plot(samples[:n, 0], samples[:n, 1], 'r-', alpha=0.5, linewidth=0.5)\n",
    "        ax_joint.scatter(samples[0, 0], samples[0, 1], color='red', s=30, label='Start')\n",
    "    \n",
    "    # If true distribution is provided, plot contours\n",
    "    if true_mean is not None and true_cov is not None:\n",
    "        # Create a grid of points\n",
    "        x = np.linspace(-3, 3, 100)\n",
    "        y = np.linspace(-3, 3, 100)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        pos = np.dstack((X, Y))\n",
    "        \n",
    "        # Compute PDF values on the grid\n",
    "        rv = stats.multivariate_normal(true_mean, true_cov)\n",
    "        Z = rv.pdf(pos)\n",
    "        \n",
    "        # Plot contours\n",
    "        levels = np.linspace(0, Z.max(), 10)[1:]\n",
    "        ax_joint.contour(X, Y, Z, levels=levels, colors='r', alpha=0.7)\n",
    "    \n",
    "    # Set labels and limits\n",
    "    ax_joint.set_xlabel('$q_1$')\n",
    "    ax_joint.set_ylabel('$q_2$')\n",
    "    ax_joint.set_xlim(-3, 3)\n",
    "    ax_joint.set_ylim(-3, 3)\n",
    "    \n",
    "    # Marginal distribution for q1\n",
    "    ax_marg_x = fig.add_subplot(gs[0, 0], sharex=ax_joint)\n",
    "    sns.kdeplot(samples[:, 0], ax=ax_marg_x, fill=True)\n",
    "    if true_mean is not None and true_cov is not None:\n",
    "        x = np.linspace(-3, 3, 1000)\n",
    "        ax_marg_x.plot(x, stats.norm.pdf(x, true_mean[0], np.sqrt(true_cov[0, 0])), 'r')\n",
    "    ax_marg_x.set_yticks([])\n",
    "    ax_marg_x.set_title(title)\n",
    "    \n",
    "    # Marginal distribution for q2\n",
    "    ax_marg_y = fig.add_subplot(gs[1, 1], sharey=ax_joint)\n",
    "    sns.kdeplot(y=samples[:, 1], ax=ax_marg_y, fill=True)\n",
    "    if true_mean is not None and true_cov is not None:\n",
    "        y = np.linspace(-3, 3, 1000)\n",
    "        ax_marg_y.plot(stats.norm.pdf(y, true_mean[1], np.sqrt(true_cov[1, 1])), y, 'r')\n",
    "    ax_marg_y.set_xticks([])\n",
    "    \n",
    "    # Turn off tick labels on the marginal plots\n",
    "    plt.setp(ax_marg_x.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax_marg_y.get_yticklabels(), visible=False)\n",
    "    \n",
    "    if show_trajectory:\n",
    "        ax_joint.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot the samples\n",
    "plot_2d_samples(samples, mean, cov, title=f\"HMC Samples (ρ={rho}, acceptance rate={acceptance_rate:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot with trajectory to show how HMC explores the space\n",
    "plot_2d_samples(samples, mean, cov, title=f\"HMC Sampling Trajectory (ρ={rho})\", show_trajectory=True, n_trajectory=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the trace plots and autocorrelation to assess mixing and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_diagnostics(samples, parameter_names=None):\n",
    "    \"\"\"\n",
    "    Plot trace plots and autocorrelation for MCMC samples.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    samples : array-like\n",
    "        MCMC samples\n",
    "    parameter_names : list, optional\n",
    "        Names of the parameters\n",
    "    \"\"\"\n",
    "    n_samples, dim = samples.shape\n",
    "    \n",
    "    if parameter_names is None:\n",
    "        parameter_names = [f\"$q_{i+1}$\" for i in range(dim)]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(dim, 2, figsize=(12, 3*dim))\n",
    "    \n",
    "    # Plot trace and autocorrelation for each parameter\n",
    "    for i in range(dim):\n",
    "        # Trace plot\n",
    "        axes[i, 0].plot(samples[:, i])\n",
    "        axes[i, 0].set_title(f\"Trace plot for {parameter_names[i]}\")\n",
    "        axes[i, 0].set_xlabel(\"Iteration\")\n",
    "        axes[i, 0].set_ylabel(parameter_names[i])\n",
    "        \n",
    "        # Autocorrelation plot\n",
    "        lags = min(50, n_samples // 5)\n",
    "        acf = np.zeros(lags)\n",
    "        for lag in range(lags):\n",
    "            acf[lag] = np.corrcoef(samples[lag:, i], samples[:-lag if lag > 0 else None, i])[0, 1]\n",
    "        \n",
    "        axes[i, 1].bar(range(lags), acf)\n",
    "        axes[i, 1].set_title(f\"Autocorrelation for {parameter_names[i]}\")\n",
    "        axes[i, 1].set_xlabel(\"Lag\")\n",
    "        axes[i, 1].set_ylabel(\"Autocorrelation\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot diagnostics\n",
    "plot_diagnostics(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Effect of Step Size and Number of Leapfrog Steps\n",
    "\n",
    "The performance of HMC depends on two key parameters: the step size $\\epsilon$ and the number of leapfrog steps $L$. Let's experiment with different values to see how they affect the acceptance rate and mixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to calculate effective sample size (ESS)\n",
    "def calculate_ess(chain):\n",
    "    \"\"\"\n",
    "    Calculate effective sample size using autocorrelation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    chain : array-like\n",
    "        MCMC chain\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ess : float\n",
    "        Effective sample size\n",
    "    \"\"\"\n",
    "    n = len(chain)\n",
    "    lags = min(50, n // 5)\n",
    "    acf = np.zeros(lags)\n",
    "    for lag in range(lags):\n",
    "        acf[lag] = np.corrcoef(chain[lag:], chain[:-lag if lag > 0 else None])[0, 1]\n",
    "    # Truncate at first negative autocorrelation\n",
    "    neg_idx = np.where(acf < 0)[0]\n",
    "    if len(neg_idx) > 0:\n",
    "        acf = acf[:neg_idx[0]]\n",
    "    # Calculate ESS\n",
    "    ess = n / (1 + 2 * np.sum(acf[1:]))\n",
    "    return ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Try different step sizes\n",
    "epsilon_values = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "L = 10  # Fixed number of leapfrog steps\n",
    "step_size_results = []\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    samples, acceptance_rate = hamiltonian_monte_carlo(\n",
    "        U, grad_U, initial_position, n_samples=2000, epsilon=epsilon, L=L, burn_in=500, thin=1\n",
    "    )\n",
    "    \n",
    "    # Calculate effective sample size\n",
    "    ess_q1 = calculate_ess(samples[:, 0])\n",
    "    ess_q2 = calculate_ess(samples[:, 1])\n",
    "    ess_avg = (ess_q1 + ess_q2) / 2\n",
    "    \n",
    "    step_size_results.append({\n",
    "        'epsilon': epsilon,\n",
    "        'acceptance_rate': acceptance_rate,\n",
    "        'ess_q1': ess_q1,\n",
    "        'ess_q2': ess_q2,\n",
    "        'ess_avg': ess_avg,\n",
    "        'efficiency': ess_avg / 2000  # ESS per iteration\n",
    "    })\n",
    "    \n",
    "    # Plot samples\n",
    "    plot_2d_samples(samples, mean, cov, \n",
    "                   title=f\"Step size ε={epsilon}, Acceptance rate={acceptance_rate:.2f}, ESS={ess_avg:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Summarize step size results\n",
    "step_size_df = pd.DataFrame(step_size_results)\n",
    "step_size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot acceptance rate and efficiency vs step size\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax1.set_xlabel('Step Size (ε)')\n",
    "ax1.set_ylabel('Acceptance Rate', color='tab:blue')\n",
    "ax1.plot(step_size_df['epsilon'], step_size_df['acceptance_rate'], 'o-', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Sampling Efficiency (ESS/N)', color='tab:red')\n",
    "ax2.plot(step_size_df['epsilon'], step_size_df['efficiency'], 'o-', color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "plt.title('Effect of Step Size on HMC Sampling')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Try different numbers of leapfrog steps\n",
    "L_values = [1, 5, 10, 20, 50]\n",
    "epsilon = 0.1  # Fixed step size\n",
    "leapfrog_steps_results = []\n",
    "\n",
    "for L in L_values:\n",
    "    samples, acceptance_rate = hamiltonian_monte_carlo(\n",
    "        U, grad_U, initial_position, n_samples=2000, epsilon=epsilon, L=L, burn_in=500, thin=1\n",
    "    )\n",
    "    \n",
    "    # Calculate effective sample size\n",
    "    ess_q1 = calculate_ess(samples[:, 0])\n",
    "    ess_q2 = calculate_ess(samples[:, 1])\n",
    "    ess_avg = (ess_q1 + ess_q2) / 2\n",
    "    \n",
    "    leapfrog_steps_results.append({\n",
    "        'L': L,\n",
    "        'acceptance_rate': acceptance_rate,\n",
    "        'ess_q1': ess_q1,\n",
    "        'ess_q2': ess_q2,\n",
    "        'ess_avg': ess_avg,\n",
    "        'efficiency': ess_avg / 2000,  # ESS per iteration\n",
    "        'efficiency_per_grad': ess_avg / (2000 * L)  # ESS per gradient evaluation\n",
    "    })\n",
    "    \n",
    "    # Plot samples\n",
    "    plot_2d_samples(samples, mean, cov, \n",
    "                   title=f\"Leapfrog steps L={L}, Acceptance rate={acceptance_rate:.2f}, ESS={ess_avg:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Summarize leapfrog steps results\n",
    "leapfrog_steps_df = pd.DataFrame(leapfrog_steps_results)\n",
    "leapfrog_steps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot acceptance rate and efficiency vs number of leapfrog steps\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax1.set_xlabel('Number of Leapfrog Steps (L)')\n",
    "ax1.set_ylabel('Acceptance Rate', color='tab:blue')\n",
    "ax1.plot(leapfrog_steps_df['L'], leapfrog_steps_df['acceptance_rate'], 'o-', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Sampling Efficiency (ESS/N)', color='tab:red')\n",
    "ax2.plot(leapfrog_steps_df['L'], leapfrog_steps_df['efficiency'], 'o-', color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "plt.title('Effect of Number of Leapfrog Steps on HMC Sampling')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot efficiency per gradient evaluation vs number of leapfrog steps\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(leapfrog_steps_df['L'], leapfrog_steps_df['efficiency_per_grad'], 'o-')\n",
    "plt.xlabel('Number of Leapfrog Steps (L)')\n",
    "plt.ylabel('Efficiency per Gradient Evaluation (ESS/N/L)')\n",
    "plt.title('Computational Efficiency vs Number of Leapfrog Steps')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Comparison with Metropolis and Gibbs Sampling\n",
    "\n",
    "Let's compare the performance of HMC with the Metropolis and Gibbs sampling algorithms for the same multivariate normal distribution with high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Metropolis algorithm\n",
    "def metropolis_sampler(log_prob_func, initial_state, n_samples, proposal_std, burn_in=0, thin=1):\n",
    "    \"\"\"\n",
    "    Metropolis algorithm for sampling from a probability distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    log_prob_func : function\n",
    "        Function that computes the log probability of a state\n",
    "    initial_state : array-like\n",
    "        Initial state of the Markov chain\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    proposal_std : float or array-like\n",
    "        Standard deviation of the Gaussian proposal distribution\n",
    "    burn_in : int, optional\n",
    "        Number of initial samples to discard\n",
    "    thin : int, optional\n",
    "        Thinning factor (keep every thin-th sample)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : array\n",
    "        Generated samples\n",
    "    acceptance_rate : float\n",
    "        Fraction of proposals that were accepted\n",
    "    \"\"\"\n",
    "    # Convert initial state to numpy array if it isn't already\n",
    "    current_state = np.array(initial_state, dtype=float)\n",
    "    dim = current_state.shape[0]  # Dimensionality of the state\n",
    "    \n",
    "    # Total number of iterations needed\n",
    "    n_iterations = burn_in + thin * n_samples\n",
    "    \n",
    "    # Initialize storage for samples and acceptance tracking\n",
    "    samples = np.zeros((n_samples, dim))\n",
    "    current_log_prob = log_prob_func(current_state)\n",
    "    n_accepted = 0\n",
    "    \n",
    "    # Main sampling loop\n",
    "    sample_idx = 0\n",
    "    for i in tqdm(range(n_iterations), desc=\"Sampling\"):\n",
    "        # Propose a new state\n",
    "        proposal = current_state + np.random.normal(0, proposal_std, size=dim)\n",
    "        \n",
    "        # Compute log probability of the proposed state\n",
    "        proposal_log_prob = log_prob_func(proposal)\n",
    "        \n",
    "        # Compute log acceptance ratio\n",
    "        log_acceptance_ratio = proposal_log_prob - current_log_prob\n",
    "        \n",
    "        # Accept or reject the proposal\n",
    "        if np.log(np.random.random()) < log_acceptance_ratio:\n",
    "            current_state = proposal\n",
    "            current_log_prob = proposal_log_prob\n",
    "            n_accepted += 1\n",
    "        \n",
    "        # Store the sample if past burn-in and due for storage based on thinning\n",
    "        if i >= burn_in and (i - burn_in) % thin == 0:\n",
    "            samples[sample_idx] = current_state\n",
    "            sample_idx += 1\n",
    "    \n",
    "    # Compute acceptance rate\n",
    "    acceptance_rate = n_accepted / n_iterations\n",
    "    \n",
    "    return samples, acceptance_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Gibbs sampler for bivariate normal\n",
    "def gibbs_sampler_bivariate_normal(mean, cov, initial_state, n_samples, burn_in=0, thin=1):\n",
    "    \"\"\"\n",
    "    Gibbs sampling for a bivariate normal distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mean : array-like\n",
    "        Mean vector [mu_1, mu_2]\n",
    "    cov : array-like\n",
    "        Covariance matrix [[sigma_1^2, rho*sigma_1*sigma_2], [rho*sigma_1*sigma_2, sigma_2^2]]\n",
    "    initial_state : array-like\n",
    "        Initial state of the Markov chain\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    burn_in : int, optional\n",
    "        Number of initial samples to discard\n",
    "    thin : int, optional\n",
    "        Thinning factor (keep every thin-th sample)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : array\n",
    "        Generated samples\n",
    "    \"\"\"\n",
    "    # Extract parameters\n",
    "    mu_1, mu_2 = mean\n",
    "    sigma_1_sq, rho_sigma_1_sigma_2, _, sigma_2_sq = cov.flatten()\n",
    "    \n",
    "    sigma_1 = np.sqrt(sigma_1_sq)\n",
    "    sigma_2 = np.sqrt(sigma_2_sq)\n",
    "    rho = rho_sigma_1_sigma_2 / (sigma_1 * sigma_2)\n",
    "    \n",
    "    # Conditional standard deviations\n",
    "    cond_std_1 = sigma_1 * np.sqrt(1 - rho**2)\n",
    "    cond_std_2 = sigma_2 * np.sqrt(1 - rho**2)\n",
    "    \n",
    "    # Convert initial state to numpy array if it isn't already\n",
    "    current_state = np.array(initial_state, dtype=float)\n",
    "    \n",
    "    # Total number of iterations needed\n",
    "    n_iterations = burn_in + thin * n_samples\n",
    "    \n",
    "    # Initialize storage for samples\n",
    "    samples = np.zeros((n_samples, 2))\n",
    "    \n",
    "    # Main sampling loop\n",
    "    sample_idx = 0\n",
    "    for i in tqdm(range(n_iterations), desc=\"Sampling\"):\n",
    "        # Sample x1 given x2\n",
    "        x2 = current_state[1]\n",
    "        cond_mean_1 = mu_1 + rho * (sigma_1 / sigma_2) * (x2 - mu_2)\n",
    "        current_state[0] = np.random.normal(cond_mean_1, cond_std_1)\n",
    "        \n",
    "        # Sample x2 given x1\n",
    "        x1 = current_state[0]\n",
    "        cond_mean_2 = mu_2 + rho * (sigma_2 / sigma_1) * (x1 - mu_1)\n",
    "        current_state[1] = np.random.normal(cond_mean_2, cond_std_2)\n",
    "        \n",
    "        # Store the sample if past burn-in and due for storage based on thinning\n",
    "        if i >= burn_in and (i - burn_in) % thin == 0:\n",
    "            samples[sample_idx] = current_state\n",
    "            sample_idx += 1\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define log probability function for Metropolis\n",
    "def log_prob(q):\n",
    "    \"\"\"Log probability for multivariate normal.\"\"\"\n",
    "    return -U(q)  # Negative potential energy\n",
    "\n",
    "# Run Metropolis sampler\n",
    "metropolis_samples, metropolis_acceptance_rate = metropolis_sampler(\n",
    "    log_prob, initial_position, n_samples=2000, proposal_std=0.3, burn_in=500, thin=1\n",
    ")\n",
    "\n",
    "# Run Gibbs sampler\n",
    "gibbs_samples = gibbs_sampler_bivariate_normal(\n",
    "    mean, cov, initial_position, n_samples=2000, burn_in=500, thin=1\n",
    ")\n",
    "\n",
    "# Run HMC sampler with optimal parameters\n",
    "hmc_samples, hmc_acceptance_rate = hamiltonian_monte_carlo(\n",
    "    U, grad_U, initial_position, n_samples=2000, epsilon=0.1, L=10, burn_in=500, thin=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate effective sample size for each method\n",
    "metropolis_ess_q1 = calculate_ess(metropolis_samples[:, 0])\n",
    "metropolis_ess_q2 = calculate_ess(metropolis_samples[:, 1])\n",
    "metropolis_ess_avg = (metropolis_ess_q1 + metropolis_ess_q2) / 2\n",
    "\n",
    "gibbs_ess_q1 = calculate_ess(gibbs_samples[:, 0])\n",
    "gibbs_ess_q2 = calculate_ess(gibbs_samples[:, 1])\n",
    "gibbs_ess_avg = (gibbs_ess_q1 + gibbs_ess_q2) / 2\n",
    "\n",
    "hmc_ess_q1 = calculate_ess(hmc_samples[:, 0])\n",
    "hmc_ess_q2 = calculate_ess(hmc_samples[:, 1])\n",
    "hmc_ess_avg = (hmc_ess_q1 + hmc_ess_q2) / 2\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Metropolis', 'Gibbs', 'HMC'],\n",
    "    'ESS q1': [metropolis_ess_q1, gibbs_ess_q1, hmc_ess_q1],\n",
    "    'ESS q2': [metropolis_ess_q2, gibbs_ess_q2, hmc_ess_q2],\n",
    "    'ESS Avg': [metropolis_ess_avg, gibbs_ess_avg, hmc_ess_avg],\n",
    "    'Efficiency (ESS/N)': [metropolis_ess_avg/2000, gibbs_ess_avg/2000, hmc_ess_avg/2000],\n",
    "    'Acceptance Rate': [metropolis_acceptance_rate, 1.0, hmc_acceptance_rate]\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot comparison of sampling methods\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot Metropolis samples\n",
    "axes[0].scatter(metropolis_samples[:, 0], metropolis_samples[:, 1], alpha=0.5, s=5)\n",
    "axes[0].set_title(f\"Metropolis (ρ={rho}, ESS={metropolis_ess_avg:.1f})\")\n",
    "axes[0].set_xlabel('$q_1$')\n",
    "axes[0].set_ylabel('$q_2$')\n",
    "axes[0].set_xlim(-3, 3)\n",
    "axes[0].set_ylim(-3, 3)\n",
    "\n",
    "# Plot Gibbs samples\n",
    "axes[1].scatter(gibbs_samples[:, 0], gibbs_samples[:, 1], alpha=0.5, s=5)\n",
    "axes[1].set_title(f\"Gibbs (ρ={rho}, ESS={gibbs_ess_avg:.1f})\")\n",
    "axes[1].set_xlabel('$q_1$')\n",
    "axes[1].set_ylabel('$q_2$')\n",
    "axes[1].set_xlim(-3, 3)\n",
    "axes[1].set_ylim(-3, 3)\n",
    "\n",
    "# Plot HMC samples\n",
    "axes[2].scatter(hmc_samples[:, 0], hmc_samples[:, 1], alpha=0.5, s=5)\n",
    "axes[2].set_title(f\"HMC (ρ={rho}, ESS={hmc_ess_avg:.1f})\")\n",
    "axes[2].set_xlabel('$q_1$')\n",
    "axes[2].set_ylabel('$q_2$')\n",
    "axes[2].set_xlim(-3, 3)\n",
    "axes[2].set_ylim(-3, 3)\n",
    "\n",
    "# Plot contours of the true distribution on all subplots\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pos = np.dstack((X, Y))\n",
    "rv = stats.multivariate_normal(mean, cov)\n",
    "Z = rv.pdf(pos)\n",
    "levels = np.linspace(0, Z.max(), 10)[1:]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.contour(X, Y, Z, levels=levels, colors='r', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot efficiency comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(comparison_df['Method'], comparison_df['Efficiency (ESS/N)'])\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Sampling Efficiency (ESS/N)')\n",
    "plt.title(f'Sampling Efficiency Comparison (ρ={rho})')\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example 2: Bayesian Logistic Regression\n",
    "\n",
    "Now, let's apply HMC to a more complex problem: Bayesian logistic regression. This is a good test case because:\n",
    "1. The posterior is not analytically tractable\n",
    "2. The gradient can be computed\n",
    "3. The model has moderate dimensionality\n",
    "4. It shows HMC's advantage in higher dimensions\n",
    "\n",
    "### 4.1 Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set true parameter values\n",
    "true_beta = np.array([0.5, 2.0, -1.0])  # Intercept and coefficients\n",
    "n_features = len(true_beta) - 1\n",
    "\n",
    "# Generate synthetic data\n",
    "n_data = 200\n",
    "X = np.random.normal(0, 1, size=(n_data, n_features))\n",
    "X_with_intercept = np.column_stack([np.ones(n_data), X])\n",
    "logits = X_with_intercept @ true_beta\n",
    "p = 1 / (1 + np.exp(-logits))\n",
    "y = np.random.binomial(1, p)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Synthetic Data for Bayesian Logistic Regression')\n",
    "plt.colorbar(label='Class')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define the Bayesian Model\n",
    "\n",
    "In Bayesian logistic regression, we model:\n",
    "\n",
    "$$p(y_i = 1 | \\mathbf{x}_i, \\boldsymbol{\\beta}) = \\frac{1}{1 + \\exp(-\\mathbf{x}_i^T \\boldsymbol{\\beta})}$$\n",
    "\n",
    "We need to define a prior for the parameters $\\boldsymbol{\\beta}$. Let's use a multivariate normal prior:\n",
    "\n",
    "$$\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$$\n",
    "\n",
    "Then, we'll sample from the posterior distribution:\n",
    "\n",
    "$$p(\\boldsymbol{\\beta} | \\mathbf{X}, \\mathbf{y}) \\propto p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}) \\cdot p(\\boldsymbol{\\beta})$$\n",
    "\n",
    "Let's define the potential energy (negative log posterior) and its gradient using autograd for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert data to autograd numpy arrays\n",
    "X_with_intercept_anp = anp.array(X_with_intercept)\n",
    "y_anp = anp.array(y)\n",
    "\n",
    "# Define potential energy (negative log posterior) using autograd\n",
    "def U_logistic(beta, X=X_with_intercept_anp, y=y_anp, sigma=10.0):\n",
    "    \"\"\"Potential energy (negative log posterior) for Bayesian logistic regression.\"\"\"\n",
    "    # Prior: beta ~ N(0, sigma^2 * I)\n",
    "    prior = anp.sum(beta**2) / (2 * sigma**2)\n",
    "    \n",
    "    # Likelihood: y ~ Bernoulli(sigmoid(X @ beta))\n",
    "    logits = X @ beta\n",
    "    log_likelihood = anp.sum(y * logits - anp.log(1 + anp.exp(logits)))\n",
    "    \n",
    "    # Negative log posterior\n",
    "    return prior - log_likelihood\n",
    "\n",
    "# Compute gradient using autograd\n",
    "grad_U_logistic = grad(U_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Sample from the Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set sampling parameters\n",
    "initial_position = np.zeros(len(true_beta))  # Start at zero\n",
    "n_samples = 5000\n",
    "epsilon = 0.01  # Step size\n",
    "L = 20  # Number of leapfrog steps\n",
    "burn_in = 1000\n",
    "thin = 2\n",
    "\n",
    "# Run the HMC sampler\n",
    "posterior_samples, acceptance_rate = hamiltonian_monte_carlo(\n",
    "    U_logistic, grad_U_logistic, initial_position, n_samples, epsilon, L, burn_in=burn_in, thin=thin\n",
    ")\n",
    "\n",
    "print(f\"Acceptance rate: {acceptance_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot diagnostics\n",
    "plot_diagnostics(posterior_samples, parameter_names=['Intercept', 'Coefficient 1', 'Coefficient 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot posterior distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i in range(3):\n",
    "    sns.histplot(posterior_samples[:, i], kde=True, ax=axes[i])\n",
    "    axes[i].axvline(true_beta[i], color='r', linestyle='--', label='True value')\n",
    "    axes[i].set_title(f'Posterior for {[\"Intercept\", \"Coefficient 1\", \"Coefficient 2\"][i]}')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Summarize posterior statistics\n",
    "posterior_summary = pd.DataFrame({\n",
    "    'Parameter': ['Intercept', 'Coefficient 1', 'Coefficient 2'],\n",
    "    'True Value': true_beta,\n",
    "    'Posterior Mean': np.mean(posterior_samples, axis=0),\n",
    "    'Posterior Median': np.median(posterior_samples, axis=0),\n",
    "    'Posterior Std': np.std(posterior_samples, axis=0),\n",
    "    '5% Quantile': np.percentile(posterior_samples, 5, axis=0),\n",
    "    '95% Quantile': np.percentile(posterior_samples, 95, axis=0)\n",
    "})\n",
    "\n",
    "posterior_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', alpha=0.7)\n",
    "\n",
    "# Create a grid of points\n",
    "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100), np.linspace(x2_min, x2_max, 100))\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "# Compute predicted probabilities for each point in the grid\n",
    "grid_with_intercept = np.column_stack([np.ones(grid.shape[0]), grid])\n",
    "\n",
    "# Use posterior mean for prediction\n",
    "beta_mean = np.mean(posterior_samples, axis=0)\n",
    "logits = grid_with_intercept @ beta_mean\n",
    "probs = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "# Reshape probabilities to grid shape\n",
    "probs = probs.reshape(xx1.shape)\n",
    "\n",
    "# Plot decision boundary (p=0.5) and probability contours\n",
    "plt.contour(xx1, xx2, probs, levels=[0.5], colors='k', linestyles='--')\n",
    "plt.contourf(xx1, xx2, probs, alpha=0.3, cmap='coolwarm')\n",
    "\n",
    "# Plot true decision boundary\n",
    "# For logistic regression, the decision boundary is a line: beta[0] + beta[1]*x1 + beta[2]*x2 = 0\n",
    "# Solving for x2: x2 = -(beta[0] + beta[1]*x1) / beta[2]\n",
    "x1_line = np.array([x1_min, x1_max])\n",
    "x2_line_true = -(true_beta[0] + true_beta[1] * x1_line) / true_beta[2]\n",
    "plt.plot(x1_line, x2_line_true, 'r-', label='True boundary')\n",
    "\n",
    "# Plot posterior mean decision boundary\n",
    "x2_line_mean = -(beta_mean[0] + beta_mean[1] * x1_line) / beta_mean[2]\n",
    "plt.plot(x1_line, x2_line_mean, 'g-', label='Posterior mean boundary')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Bayesian Logistic Regression: Decision Boundary')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot posterior uncertainty in decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', alpha=0.7)\n",
    "\n",
    "# Plot a sample of decision boundaries from the posterior\n",
    "n_samples_to_plot = 100\n",
    "sample_indices = np.random.choice(len(posterior_samples), n_samples_to_plot, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    beta_sample = posterior_samples[idx]\n",
    "    x2_line_sample = -(beta_sample[0] + beta_sample[1] * x1_line) / beta_sample[2]\n",
    "    plt.plot(x1_line, x2_line_sample, 'b-', alpha=0.05)\n",
    "\n",
    "# Plot true decision boundary\n",
    "plt.plot(x1_line, x2_line_true, 'r-', linewidth=2, label='True boundary')\n",
    "\n",
    "# Plot posterior mean decision boundary\n",
    "plt.plot(x1_line, x2_line_mean, 'g-', linewidth=2, label='Posterior mean boundary')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Bayesian Logistic Regression: Posterior Uncertainty in Decision Boundary')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Comparison with Metropolis Algorithm\n",
    "\n",
    "Let's compare the performance of HMC with the Metropolis algorithm for this logistic regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define log probability function for Metropolis\n",
    "def log_prob_logistic(beta):\n",
    "    \"\"\"Log probability for Bayesian logistic regression.\"\"\"\n",
    "    return -U_logistic(beta)\n",
    "\n",
    "# Run Metropolis sampler\n",
    "metropolis_samples, metropolis_acceptance_rate = metropolis_sampler(\n",
    "    log_prob_logistic, initial_position, n_samples=5000, proposal_std=0.05, burn_in=1000, thin=2\n",
    ")\n",
    "\n",
    "print(f\"Metropolis acceptance rate: {metropolis_acceptance_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot diagnostics for Metropolis\n",
    "plot_diagnostics(metropolis_samples, parameter_names=['Intercept', 'Coefficient 1', 'Coefficient 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate effective sample size for each method\n",
    "metropolis_ess = np.array([calculate_ess(metropolis_samples[:, i]) for i in range(3)])\n",
    "hmc_ess = np.array([calculate_ess(posterior_samples[:, i]) for i in range(3)])\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Parameter': ['Intercept', 'Coefficient 1', 'Coefficient 2'],\n",
    "    'Metropolis ESS': metropolis_ess,\n",
    "    'Metropolis Efficiency (ESS/N)': metropolis_ess / len(metropolis_samples),\n",
    "    'HMC ESS': hmc_ess,\n",
    "    'HMC Efficiency (ESS/N)': hmc_ess / len(posterior_samples),\n",
    "    'Efficiency Ratio (HMC/Metropolis)': hmc_ess / metropolis_ess\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot efficiency comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, comparison_df['Metropolis Efficiency (ESS/N)'], width, label='Metropolis')\n",
    "plt.bar(x + width/2, comparison_df['HMC Efficiency (ESS/N)'], width, label='HMC')\n",
    "\n",
    "plt.xlabel('Parameter')\n",
    "plt.ylabel('Sampling Efficiency (ESS/N)')\n",
    "plt.title('Sampling Efficiency: Metropolis vs HMC for Logistic Regression')\n",
    "plt.xticks(x, comparison_df['Parameter'])\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Limitations and Extensions of HMC\n",
    "\n",
    "### 5.1 Limitations\n",
    "\n",
    "Hamiltonian Monte Carlo, while powerful, has several limitations:\n",
    "\n",
    "1. **Requires Gradient Information**: HMC requires the gradient of the log probability, which may not always be available or may be computationally expensive to compute.\n",
    "\n",
    "2. **Tuning Parameters**: HMC has two main tuning parameters (step size and number of leapfrog steps) that need to be carefully chosen for optimal performance.\n",
    "\n",
    "3. **Computational Cost**: Each HMC iteration requires multiple gradient evaluations (one for each leapfrog step), making it more computationally expensive than simpler methods like Metropolis or Gibbs sampling.\n",
    "\n",
    "4. **Difficulty with Discrete Parameters**: HMC is designed for continuous parameters and cannot be directly applied to discrete parameters.\n",
    "\n",
    "5. **Sensitivity to Ill-Conditioned Posteriors**: HMC can struggle with ill-conditioned posteriors where the scales of different parameters vary widely.\n",
    "\n",
    "### 5.2 Extensions and Alternatives\n",
    "\n",
    "Several extensions and alternatives address these limitations:\n",
    "\n",
    "1. **No-U-Turn Sampler (NUTS)**: An adaptive version of HMC that automatically tunes the number of leapfrog steps, eliminating the need to manually specify this parameter.\n",
    "\n",
    "2. **Riemannian HMC**: Uses a position-dependent mass matrix to better handle ill-conditioned posteriors.\n",
    "\n",
    "3. **Dual Averaging**: A method for automatically tuning the step size to achieve a target acceptance rate.\n",
    "\n",
    "4. **Hamiltonian Monte Carlo with Constraints**: Extensions of HMC that can handle constraints on the parameter space.\n",
    "\n",
    "5. **Discontinuous HMC**: Variants of HMC that can handle discontinuities in the target distribution.\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "In this notebook, we've explored the Hamiltonian Monte Carlo algorithm, an advanced MCMC method that uses concepts from physics to efficiently sample from complex probability distributions. We've implemented it from scratch and applied it to sample from multivariate normal distributions and to perform Bayesian logistic regression.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. HMC uses gradient information to propose states that are distant from the current state but still have a high probability of acceptance, reducing random walk behavior.\n",
    "\n",
    "2. The performance of HMC depends on two key parameters: the step size and the number of leapfrog steps. These need to be carefully tuned for optimal performance.\n",
    "\n",
    "3. HMC is particularly effective for high-dimensional problems with complex geometries, where simpler methods like Metropolis and Gibbs sampling can struggle.\n",
    "\n",
    "4. The main drawback of HMC is its computational cost, as each iteration requires multiple gradient evaluations.\n",
    "\n",
    "5. Extensions like NUTS address some of the limitations of basic HMC by automatically tuning the parameters.\n",
    "\n",
    "In the next notebook, we'll explore how to implement these examples using PyMC, which provides efficient implementations of HMC and its extensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
